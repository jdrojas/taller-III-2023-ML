{
  "hash": "7e3c20ea7a24195f090dff9bbd2626b9",
  "result": {
    "engine": "jupyter",
    "markdown": "# Capítulo 5\n## 5.2:Selección de algoritmos\n\nElegir un algoritmo puede ser una tarea difícil. En el caso de que se disponga de mucho tiempo se pueden probar varios, sin embargo no siempre es el caso, por lo que hay una serie de preguntas que se pueden realizar con el fin de hacer el proceso más eficiente.\n\n### Explicabilidad o interpretabilidad\n\nCon que facilidad el algoritmo logra explicar las predicciones que realiza, por lo general un modelo que realiza una predicción específica es dificíl de entender y aún más de explicar. Algunos ejemplos son los modelos de redes neuronales o el método de emsamble. Estos algoritmos que carecen de tal explicación se llaman algoritmo de caja negra.Por otro lado, los algoritmos de aprendizaje kNN, regresión lineal o árbol de decisión producen modelos que no siempre son los más precisos, sin embargo, la forma en que hacen su predicción es muy sencilla.\n\n**Ejemplo**\n\n<style>\n  /* Estilo para centrar la imagen */\n  img {\n    display: block;\n    margin-left: auto;\n    margin-right: auto;\n  }\n \n  /* Estilo para centrar el nombre de la imagen */\n  figcaption {\n    text-align: center;\n  }\n </style>\n \n  ![**Figura 1**. Método de emsamble](https://machinelearningmastery.com/wp-content/uploads/2020/07/Example-of-Combining-Decision-Boundaries-Using-an-Ensemble-768x573.png)\n\n\n\n\n <!-- ![**Figura 2**. Método redes neuronales](red.ppm) -->\n\n### Requisitos de memoria\n\nSi el conjunto de datos se puede cargar de forma completa en la memoria RAM del servidor o computador, entonces la disponibilidad de logaritmos es amplia. Sin embargo, si ese no es el caso, se debe optar por logaritmos de aprendizaje incremental, estos pueden mejorar el modelo añadiendo más datos gradualmente, básicamente se adaptan a nuevos nuevos sin el olvidar la información ya existente.\n\n### Número de funciones y característica\n\nAlgunos algoritmos, como las redes neuronales y el descenso de gradiente, pueden manejar un gran número de ejemplos y millones de características. Otros, como SVM, pueden ser muy modestos en su capacidad. Entonces, a la hora de escoger un logaritmo se debe considerar el tamaño de los datos y la cantidad de funciones.\n\n### Características categóricas frente a numéricas\n\nAlgunos algoritmos solo pueden funcionar con datos numéricos, por lo que si se tienen datos en un formato categórico o no numérico, se deberá considerar un proceso para convertirlos en datos numéricos mediante técnicas como la codificación one-hot.\n\n### linealidad de los datos\n\nSi los datos son linealmente separables o pueden modelarse mediante un modelo lineal, se puede utilizar SVM, regresión logística o la regresión lineal, si no es el caso las redes neuronales o los algoritmos de conjunto, son una mejor opción.\n\n**Ejemplo**\n\n<!-- ![**Figura 3**. Métodos lineales](Lineal.png) -->\n\n\n\n\n\n\n<!-- ![**Figura 4**. No linealidad datos](nolineal.ppm) -->\n\n\n### Velocidad de entrenamiento\n\nEs el tiempo que tarda un algoritmo en aprender y crear un modelo. Las redes neuronales son conocidas por la considerable cantidad de tiempo que requieren para entrenar un modelo. Los algoritmos de máquina tradicionales como K-Vecinos más cercanos y Regresión logística toman mucho menos tiempo. Algunos algoritmos, como Bosque aleatorio, requieren diferentes tiempos de entrenamiento según los núcleos de CPU que se utilizan.\n\n**Ejemplo**\n\n<!-- ![**Figura 5**. Bosque aleatorio](forest.png) -->\n\n### Velocidad de predicción\n\nTiempo que le toma a un modelo hacer sus predicciones, en este caso se debe considerar qué tan rápido debe ser el modelo a la hora de generar predicciones y para que función se está utilizando el modelo escogido. Si no se quiere adivinar cuál es el mejor algoritmo para los datos, una forma de elegir es utilizar la prueba de validación.\n\n**Ejemplo**\n\n![**Figura 6**. Diagrama Selección del algoritmo](https://miro.medium.com/v2/resize:fit:640/1*2NR51X0FDjLB13u4WdYc4g.png)\n\n## 5.3:Three sets\n\nEn Machine Learning, para el estudio y construcción de algoritmos que sean capaces de aprender de los datos y hacer predicciones utilizando esa información, se utiliza la construcción de un modelo matemático a partir de datos de entrada y estos datos para ser utilizados se dividen en conjuntos de datos, estos son: \n\n1. Conjunto de entrenamiento\n2. Conjunto de validación\n3. Conjunto de prueba\n\nEl conjunto de entrenamiento, suele ser el conjunto más grande y es el que se utiliza para construir el modelo, los conjuntos de validación y prueba suelen tener el mismo tamaño y esto son menores que en el conjunto de entrenamiento , tanto los conjuntos de validación y prueba no son usados para construir el modelo. A estos conjuntos se les suele llamar conjuntos esfera. \n\n**Ejemplo**\n\n<!-- ![**Figura 7**. Gráfico de proporciones](Gr%C3%A1fico.png) -->\n\n\n![**Figura 8**. Solución problemas](https://enriquecatala.com/img/posts/IOB_train_test_split/train_test.png) \n\n\n\n\n\n# Ajuste de hiperparámetros\nEl analista de datos debe seleccionar buenos hiperparámetros para el algoritmo que se esté trabajando. Como se sabe que lkos hiperparámetros no son optimizados por el algoritmo de aprendizaje por si solo; el analista debe ajustar la mejor combinación de hiperparámetros.\n\nUna forma usual de realizar esto es, si se tienen suficientes datos para tener un conjunto de validación decente (cada calse tiene al menos un par de docenas de ejemplos) y el número de hiperparámetros y sus rangos no son muy grandes; se puede utilizar **grid search**. Este método es la más simple estrategia para ajustar los hiperparámetros.\n\nAsuma que entrenamos un SVM y queremos ajustar dos hiperparámetros: el parámetro de penalización $C$ (un número real positivo) y el kernel (si es 'linear' o 'rbf'). Si es la primera vez trabajando con el datset y no tenemos un rango de valores para $C$, un truco muy común es usar una escala logarítimica; entonces podemos tomar los valores de $[0.001, 0.01, 0.1, 1.0, 10, 100, 1000]$. Note que debido lo anterior tenemos 14 combinaciones de hiperparámetros diferentes: [(0.001, 'linear'), (0.01, 'linear'), (0.1, 'linear'), (1.0, 'linear'), (10, 'linear'), (100, 'linear'), (1000, 'linear'), (0.001, 'rbf'), (0.01, 'rbf'), (0.1, 'rbf'), (1.0, 'rbf'), (10, 'rbf'), (100, 'rbf'), (1000, 'rbf')]\n\nLo que se hace es tomar el conujunto de entrenamiento y entrenarlo con los 14 modelos diferentes. Luego, se evalúa el rendimiento de cada modelo con los datos de validación usando alguna métrica. Y se elige el que tenga la mejor métrica. Y por último se evalúa con el conjunto de prueba.\n\nUn problema es el consumo de tiempo. Para ello, hay técnicas más eficientes como **random search** y **bayesian hyperparameter optimization**.\n\nEn el random search, uno le da una una distribución estadística para cada hiperparámetro y el número total de combinaciones que ser quieren realizar. La técnica bayesiana utiliza los resultados anteriores para elegir los próximos valores para evaluar.\n\n## Validación Cruzada\n\nCuando no se tienen muchos datos para ajustar los hiperparámetros. Entonces, podemos dividir el conjunto de entrenamiento en varios subconjuntos (*fold*) del mismo tamaño, lo usual es usar 5 folds. Así, se dividen los datos de entrenamiento en 5 folds $\\{F_1, F_2, F_3, F_4, F_5\\}$ cada una contiene el $20\\%$ de los datos de entrenamiento. Así, se entrenan 5 modelos, de la siguiente forma: para el primer modelo $f_1$ se utilizan los folds $F_2, F_3, F_4, F_5$ y $F_1$ se utiliza como conjunto de validación; para el segundo modelo $f_2$ se utilizan los folds $F_1, F_3, F_4, F_5$ y el $F_2$ es el conjutno de validación; y aspi hasta completar $f_5$.\n\nSe puede aplicar grid search con la validación cruzada para encontrar los mejores valores para los hiperparámetro de nuestro modelo. \n\n## Ejemplo \n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\ndigits = datasets.load_digits()\nn_samples = len(digits.images)\nX = digits.images.reshape((n_samples, -1))\ny = digits.target == 8\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n```\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport pandas as pd\n\n\ndef print_dataframe(filtered_cv_results):\n    \"\"\"Pretty print for filtered dataframe\"\"\"\n    for mean_precision, std_precision, mean_recall, std_recall, params in zip(\n        filtered_cv_results[\"mean_test_precision\"],\n        filtered_cv_results[\"std_test_precision\"],\n        filtered_cv_results[\"mean_test_recall\"],\n        filtered_cv_results[\"std_test_recall\"],\n        filtered_cv_results[\"params\"],\n    ):\n        print(\n            f\"precision: {mean_precision:0.3f} (±{std_precision:0.03f}),\"\n            f\" recall: {mean_recall:0.3f} (±{std_recall:0.03f}),\"\n            f\" for {params}\"\n        )\n    print()\n\n\ndef refit_strategy(cv_results):\n    \"\"\"Define the strategy to select the best estimator.\n\n    The strategy defined here is to filter-out all results below a precision threshold\n    of 0.98, rank the remaining by recall and keep all models with one standard\n    deviation of the best by recall. Once these models are selected, we can select the\n    fastest model to predict.\n\n    Parameters\n    ----------\n    cv_results : dict of numpy (masked) ndarrays\n        CV results as returned by the `GridSearchCV`.\n\n    Returns\n    -------\n    best_index : int\n        The index of the best estimator as it appears in `cv_results`.\n    \"\"\"\n    # print the info about the grid-search for the different scores\n    precision_threshold = 0.98\n\n    cv_results_ = pd.DataFrame(cv_results)\n    print(\"All grid-search results:\")\n    print_dataframe(cv_results_)\n\n    # Filter-out all results below the threshold\n    high_precision_cv_results = cv_results_[\n        cv_results_[\"mean_test_precision\"] > precision_threshold\n    ]\n\n    print(f\"Models with a precision higher than {precision_threshold}:\")\n    print_dataframe(high_precision_cv_results)\n\n    high_precision_cv_results = high_precision_cv_results[\n        [\n            \"mean_score_time\",\n            \"mean_test_recall\",\n            \"std_test_recall\",\n            \"mean_test_precision\",\n            \"std_test_precision\",\n            \"rank_test_recall\",\n            \"rank_test_precision\",\n            \"params\",\n        ]\n    ]\n\n    # Select the most performant models in terms of recall\n    # (within 1 sigma from the best)\n    best_recall_std = high_precision_cv_results[\"mean_test_recall\"].std()\n    best_recall = high_precision_cv_results[\"mean_test_recall\"].max()\n    best_recall_threshold = best_recall - best_recall_std\n\n    high_recall_cv_results = high_precision_cv_results[\n        high_precision_cv_results[\"mean_test_recall\"] > best_recall_threshold\n    ]\n    print(\n        \"Out of the previously selected high precision models, we keep all the\\n\"\n        \"the models within one standard deviation of the highest recall model:\"\n    )\n    print_dataframe(high_recall_cv_results)\n\n    # From the best candidates, select the fastest model to predict\n    fastest_top_recall_high_precision_index = high_recall_cv_results[\n        \"mean_score_time\"\n    ].idxmin()\n\n    print(\n        \"\\nThe selected final model is the fastest to predict out of the previously\\n\"\n        \"selected subset of best models based on precision and recall.\\n\"\n        \"Its scoring time is:\\n\\n\"\n        f\"{high_recall_cv_results.loc[fastest_top_recall_high_precision_index]}\"\n    )\n\n    return fastest_top_recall_high_precision_index\n```\n:::\n\n\nAhora, se define el grid search\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nscores = ['precision', 'recall']\n\ntuned_params = [\n    {'kernel':['rbf'], 'gamma':[1e-3, 1e-4], 'C':[1, 10, 100, 1000]},\n    {'kernel':['linear'], 'C':[1, 10, 100, 1000]}\n]\n\ngrid_search = GridSearchCV(SVC(), tuned_params, scoring=scores, refit=refit_strategy)\ngrid_search.fit(X_train, y_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAll grid-search results:\nprecision: 1.000 (±0.000), recall: 0.854 (±0.063), for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\nprecision: 1.000 (±0.000), recall: 0.257 (±0.061), for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\nprecision: 1.000 (±0.000), recall: 0.877 (±0.069), for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\nprecision: 0.968 (±0.039), recall: 0.780 (±0.083), for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\nprecision: 1.000 (±0.000), recall: 0.877 (±0.069), for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\nprecision: 0.905 (±0.058), recall: 0.889 (±0.074), for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\nprecision: 1.000 (±0.000), recall: 0.877 (±0.069), for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\nprecision: 0.904 (±0.058), recall: 0.890 (±0.073), for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\nprecision: 0.695 (±0.073), recall: 0.743 (±0.065), for {'C': 1, 'kernel': 'linear'}\nprecision: 0.643 (±0.066), recall: 0.757 (±0.066), for {'C': 10, 'kernel': 'linear'}\nprecision: 0.611 (±0.028), recall: 0.744 (±0.044), for {'C': 100, 'kernel': 'linear'}\nprecision: 0.618 (±0.039), recall: 0.744 (±0.044), for {'C': 1000, 'kernel': 'linear'}\n\nModels with a precision higher than 0.98:\nprecision: 1.000 (±0.000), recall: 0.854 (±0.063), for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\nprecision: 1.000 (±0.000), recall: 0.257 (±0.061), for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\nprecision: 1.000 (±0.000), recall: 0.877 (±0.069), for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\nprecision: 1.000 (±0.000), recall: 0.877 (±0.069), for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\nprecision: 1.000 (±0.000), recall: 0.877 (±0.069), for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n\nOut of the previously selected high precision models, we keep all the\nthe models within one standard deviation of the highest recall model:\nprecision: 1.000 (±0.000), recall: 0.854 (±0.063), for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\nprecision: 1.000 (±0.000), recall: 0.877 (±0.069), for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\nprecision: 1.000 (±0.000), recall: 0.877 (±0.069), for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\nprecision: 1.000 (±0.000), recall: 0.877 (±0.069), for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n\n\nThe selected final model is the fastest to predict out of the previously\nselected subset of best models based on precision and recall.\nIts scoring time is:\n\nmean_score_time                                          0.008195\nmean_test_recall                                         0.877206\nstd_test_recall                                          0.069196\nmean_test_precision                                           1.0\nstd_test_precision                                            0.0\nrank_test_recall                                                3\nrank_test_precision                                             1\nparams                 {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\nName: 2, dtype: object\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\nGridSearchCV(estimator=SVC(),\n             param_grid=[{'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001],\n                          'kernel': ['rbf']},\n                         {'C': [1, 10, 100, 1000], 'kernel': ['linear']}],\n             refit=<function refit_strategy at 0x130c30900>,\n             scoring=['precision', 'recall'])\n```\n:::\n:::\n\n\n",
    "supporting": [
      "4_practica_basica_files/figure-pdf"
    ],
    "filters": []
  }
}
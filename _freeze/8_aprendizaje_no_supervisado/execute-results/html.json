{
  "hash": "8f53577c675bb63f182de8184605bd83",
  "result": {
    "engine": "jupyter",
    "markdown": "# Aprendizaje no supervisado\n\n## Estimación de densidad\n\nLa estimación de la densidad es un problema de modelar la función de densidad (*pdf*) de la distribución desconcida del dataset. Sus aplicaciones principales son en la ditección de novedades e intrusiones.\nAnteriormente se trabajó con la estimación de la *pdf* para el caso paramétrico con la distribución normal multivariada. Acá usaremos el método del kernel, ya que este es no paramétrico.\n\nAsí, sea $\\{x_i\\}_{i=1}^{N}$ un dataset de una dimensión donde las muestras son construidas a partir de una *pdf* desconocida $f$ con $x_i\\in\\mathbb{R}, \\ \\forall i=1, \\ldots, N$. Estamos interesados en modelar la curva de la función $f$. Con nuestro modelo de kernel, denotado por $\\hat{f}$, defindo como:\n$$\n\\hat{f}_h(x) = \\dfrac{1}{Nh}\\sum_{i=1}^{N} k\\left(\\dfrac{x-x_i}{h}\\right)\n$${#eq-fhat}\n\nDonde $h$ es un hiperparámetro que controla la relación sesgo-varianza. Acá usaremos el kernel gaussiano:\n\\begin{equation}\nk(z) = \\dfrac{1}{\\sqrt{2\\pi}}\\exp\\left(\\dfrac{-z^2}{2}\\right)\n\\end{equation}\nNosotros buscamos el valor de $h$ que minimiza la diferencia entre la curva original $f$ y la curva aproximada de nuestro modelo $f_{h}$. Una medida razonable para esta diferencia es el error cuadrático medio integrado (MISE, por sus siglas en inglés), definido por:\n$$\nMISE(b) = \\mathbb{E}\\left[\\int_{\\mathbb{R}}\\left(\\hat{f}_{h}(x)-f(x)\\right)^2dx\\right]\n$${#eq-mise}\n\nEn la ecuación (@eq-mise) la integral $\\int_{\\mathbb{R}}$ remplaza a la sumatoria $\\displaystyle\\sum_{i=1}^{N}$ que empleamos en el promedio, mientras que la esperanza $\\mathbb{E}$ reemplaza el promedio $\\dfrac{1}{N}$.\n\nNotese que cuando la función de pérdida es continua como la función de $\\left(\\hat{f}_{h}(x)-f(x)\\right)^2$, se reemplaza la sumatoria por la integrasl. El operador de esperanza $\\mathbb{E}$ siginifica que queremos que $h$ sea el óptimo para todos las posibilidades del set de entrenamiento. Esto es importante debido a que $\\hat{f}_{h}$ es definido en un conjunto finito de datos de alguna distribución de probabilidad; mientras que la *pdf* real $f$ está definida en un dominio infinito $\\mathbb{R}$.\n\nNote que, reescribiendo el lado derecho de la (@eq-mise), obtenemos\n\\begin{equation*}\n\\mathbb{E}\\left[\\int_{\\mathbb{R}}\\hat{f}_{h}^{2}(x)dx\\right]\n-2\\mathbb{E}\\left[\\int_{\\mathbb{R}}\\hat{f}_{h}(x)f(x)dx\\right]\n+ \\mathbb{E}\\left[\\int_{\\mathbb{R}}f^{2}(x)dx\\right]\n\\end{equation*}\n\nNote que el tercer término es independiente de $h$ y podría ser ignorado. Un estimador insesgado del primer término está dado por $\\int_{\\mathbb{R}}\\hat{f}_{b}^{2}(x)dx$, mientras que el estimador insesgado para el segundo término está aproximado por $\\dfrac{-2}{N}\\displaystyle\\sum_{i=1}^{N}\\hat{f}_{h}^{(i)}(x_i)$, donde $\\hat{f}_{h}^{(i)}(x_i)$ es el kernel con los datos de entrenamiento menos el dato $x_i$.\n\nEl término $\\displaystyle\\sum_{i=1}^{N}\\hat{f}_{h}^{(i)}(x_i)$ es conocindo como el estimador de dejar una estimación por fuera (*leave one out estimate*); es una forma de validación cruzada donde cada *fold* contienen una muestra.\nAdemás, se puede ver como $\\int_{\\mathbb{R}}\\hat{f}_{h}(x)f(x)dx$ es la esperanza de la función $\\hat{f}_{h}$, esto por que $f$ es una función de densidad. Se puede demostra que el estimador *leave one out estimate* es un estimador insesgado para $\\mathbb{E}\\left[\\int_{\\mathbb{R}}\\hat{f}_{h}(x)f(x)dx\\right]$.\n\nAhora, para hallar el valor óptimo $h^*$ para $h$, queremos minimizar la función de costo definida por:\n$$\n\\displaystyle\\int_{\\mathbb{R}}\\hat{f}*{h}^{2}(x)dx - \\dfrac{2}{N}\\displaystyle\\sum*{i=1}^{N}\\hat{f}_{h}^{(i)}(x_i)\n$$\n\nSe puede hallar $h^*$ utilizando *grid search*¨. Para $D$ dimensiones, el término del error $x-x_i$ de la (@eq-fhat) puede ser reemplazado por la norma euclidea $||\\mathbb{x}-\\mathbb{x}_{i}||$.\n\n::: {#a1480788 .cell execution_count=1}\n``` {.python .cell-code}\n#Importar las librerías\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import norm\nfrom sklearn.neighbors import KernelDensity\n\n# ----------------------------------------------------------------------\n# Plot the progression of histograms to kernels\nnp.random.seed(1)\nN = 20\nX = np.concatenate(\n    (np.random.normal(0, 1, int(0.3 * N)), np.random.normal(5, 1, int(0.7 * N)))\n)[:, np.newaxis]\nX_plot = np.linspace(-5, 10, 1000)[:, np.newaxis]\nbins = np.linspace(-5, 10, 10)\n\nfig, ax = plt.subplots(2, 2, sharex=True, sharey=True)\nfig.subplots_adjust(hspace=0.05, wspace=0.05)\n\n# Histograma\nax[0, 0].hist(X[:, 0], bins=bins, fc=\"#AAAAFF\", density=True)\nax[0, 0].text(-3.5, 0.31, \"Histograma\")\n\n# Histograma con las particiones desplazadas\nax[0, 1].hist(X[:, 0], bins=bins + 0.75, fc=\"#AAAAFF\", density=True)\nax[0, 1].text(-3.5, 0.31, \"Histograma, bins desplazados\")\n\n# tophat KDE\nkde = KernelDensity(kernel=\"tophat\", bandwidth=0.75).fit(X)\nlog_dens = kde.score_samples(X_plot)\nax[1, 0].fill(X_plot[:, 0], np.exp(log_dens), fc=\"#AAAAFF\")\nax[1, 0].text(-3.5, 0.31, \"Tophat Kernel Density\")\n\n# Gaussian KDE\nkde = KernelDensity(kernel=\"gaussian\", bandwidth=0.75).fit(X)\nlog_dens = kde.score_samples(X_plot)\nax[1, 1].fill(X_plot[:, 0], np.exp(log_dens), fc=\"#AAAAFF\")\nax[1, 1].text(-3.5, 0.31, \"Gaussian Kernel Density\")\n\nfor axi in ax.ravel():\n    axi.plot(X[:, 0], np.full(X.shape[0], -0.01), \"+k\")\n    axi.set_xlim(-4, 9)\n    axi.set_ylim(-0.02, 0.34)\n\nfor axi in ax[:, 0]:\n    axi.set_ylabel(\"Normalized Density\")\n\nfor axi in ax[1, :]:\n    axi.set_xlabel(\"x\")\n\n# ----------------------------------------------------------------------\n# Plot all available kernels\nX_plot = np.linspace(-6, 6, 1000)[:, None]\nX_src = np.zeros((1, 1))\n\nfig, ax = plt.subplots(2, 3, sharex=True, sharey=True)\nfig.subplots_adjust(left=0.05, right=0.95, hspace=0.05, wspace=0.05)\n\n\ndef format_func(x, loc):\n    if x == 0:\n        return \"0\"\n    elif x == 1:\n        return \"h\"\n    elif x == -1:\n        return \"-h\"\n    else:\n        return \"%ih\" % x\n\n\nfor i, kernel in enumerate(\n    [\"gaussian\", \"tophat\", \"epanechnikov\", \"exponential\", \"linear\", \"cosine\"]\n):\n    axi = ax.ravel()[i]\n    log_dens = KernelDensity(kernel=kernel).fit(X_src).score_samples(X_plot)\n    axi.fill(X_plot[:, 0], np.exp(log_dens), \"-k\", fc=\"#AAAAFF\")\n    axi.text(-2.6, 0.95, kernel)\n\n    axi.xaxis.set_major_formatter(plt.FuncFormatter(format_func))\n    axi.xaxis.set_major_locator(plt.MultipleLocator(1))\n    axi.yaxis.set_major_locator(plt.NullLocator())\n\n    axi.set_ylim(0, 1.05)\n    axi.set_xlim(-2.9, 2.9)\n\nax[0, 1].set_title(\"Kernels Disponibles\")\n\n# ----------------------------------------------------------------------\n# Plot a 1D density example\nN = 100\nnp.random.seed(1)\nX = np.concatenate(\n    (np.random.normal(0, 1, int(0.3 * N)), np.random.normal(5, 1, int(0.7 * N)))\n)[:, np.newaxis]\n\nX_plot = np.linspace(-5, 10, 1000)[:, np.newaxis]\n\ntrue_dens = 0.3 * norm(0, 1).pdf(X_plot[:, 0]) + 0.7 * norm(5, 1).pdf(X_plot[:, 0])\n\nfig, ax = plt.subplots()\nax.fill(X_plot[:, 0], true_dens, fc=\"black\", alpha=0.2, label=\"input distribution\")\ncolors = [\"navy\", \"cornflowerblue\", \"darkorange\"]\nkernels = [\"gaussian\", \"tophat\", \"epanechnikov\"]\nlw = 2\n\nfor color, kernel in zip(colors, kernels):\n    kde = KernelDensity(kernel=kernel, bandwidth=0.5).fit(X)\n    log_dens = kde.score_samples(X_plot)\n    ax.plot(\n        X_plot[:, 0],\n        np.exp(log_dens),\n        color=color,\n        lw=lw,\n        linestyle=\"-\",\n        label=\"kernel = '{0}'\".format(kernel),\n    )\n\nax.text(6, 0.38, \"N={0} points\".format(N))\n\nax.legend(loc=\"upper left\")\nax.plot(X[:, 0], -0.005 - 0.01 * np.random.random(X.shape[0]), \"+k\")\n\nax.set_xlim(-4, 9)\nax.set_ylim(-0.02, 0.4)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](8_aprendizaje_no_supervisado_files/figure-html/cell-2-output-1.png){width=589 height=429}\n:::\n\n::: {.cell-output .cell-output-display}\n![](8_aprendizaje_no_supervisado_files/figure-html/cell-2-output-2.png){width=624 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](8_aprendizaje_no_supervisado_files/figure-html/cell-2-output-3.png){width=579 height=416}\n:::\n:::\n\n\nLa agrupación es un problema de aprender a asignar una etiqueta a ejemplos aprovechando un no etiquetado conjunto de datos. Debido a que el conjunto de datos no está etiquetado en absoluto, decidir si el modelo aprendido es óptimo es mucho más complicado que en el aprendizaje supervisado. Existe una variedad de algoritmos de agrupamiento y, desafortunadamente, es difícil saber cuál es el mejor calidad para su conjunto de datos. Generalmente, el rendimiento de cada algoritmo depende de las propiedades desconocidas de la distribución de probabilidad de la que se extrajo el conjunto de datos.\n\n## K-Medias\n\nEl algoritmo de agrupamiento de k-medias funciona de la siguiente manera:\n\nPrimero, el analista tiene que elegir k - el número de clases (o grupos). Luego colocamos aleatoriamente k vectores de características, llamados centroides, en el espacio de características.\n\nLuego calculamos la distancia desde cada ejemplo x a cada centroide usando alguna métrica, como la distancia euclidiana. Luego asignamos el centroide más cercano a cada ejemplo (como si etiquetáramos cada ejemplo con una identificación de centroide como etiqueta). Para cada centroide, calculamos el vector de características promedio de los ejemplos etiquetados con él. Estas características promedio los vectores se convierten en las nuevas ubicaciones de los centroides.\n\nEl valor de k, el número de clusters, es un hiperparámetro que los datos deben ajustar. Existen algunas técnicas para seleccionar k. Ninguno de ellos ha demostrado ser óptimo. La mayoría de requieren que el analista haga una “suposición fundamentada” observando algunas métricas o examinando visualmente las asignaciones de grupos. Más adelante en este capítulo, consideraremos una técnica lo que permite elegir un valor razonablemente bueno para k sin mirar los datos y hacer suposiciones.\n\n### Ejemplo\n\nVisualización de datos\n\n::: {#23394af9 .cell execution_count=2}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\nx = [4, 5, 10, 4, 3, 11, 14 , 6, 10, 12]\ny = [21, 19, 24, 17, 16, 25, 24, 22, 21, 21]\n\nplt.scatter(x, y)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](8_aprendizaje_no_supervisado_files/figure-html/cell-3-output-1.png){width=566 height=411}\n:::\n:::\n\n\nMétodo del codo para seleccionar la cantidad de k:\n\n::: {#d03efe8b .cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.cluster import KMeans\n\ndata = list(zip(x, y))\ninertias = []\n\nfor i in range(1,11):\n    kmeans = KMeans(n_clusters=i, n_init=\"auto\")\n    kmeans.fit(data)\n    inertias.append(kmeans.inertia_)\n\nplt.plot(range(1,11), inertias, marker='o')\nplt.title('Elbow method')\nplt.xlabel('Number of clusters')\nplt.ylabel('Inertia')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](8_aprendizaje_no_supervisado_files/figure-html/cell-4-output-1.png){width=593 height=449}\n:::\n:::\n\n\n::: {#f441ca43 .cell execution_count=4}\n``` {.python .cell-code}\nkmeans = KMeans(n_clusters=2, n_init=\"auto\")\nkmeans.fit(data)\n\nplt.scatter(x, y, c=kmeans.labels_)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](8_aprendizaje_no_supervisado_files/figure-html/cell-5-output-1.png){width=566 height=411}\n:::\n:::\n\n\n## Reducción de dimensionalidad\n\n### Análisis de componentes principales (PCA)\n\n### UMAP\n\n UMAP (Uniform Manifold Approximation and Projection) es un algoritmo de aprendizaje de manifolds para la reducción de dimensionalidad, superior a t-SNE por su eficiencia y versatilidad. Desarrollado por Leland McInnes, John Healy y James Melville en 2018, UMAP se fundamenta en el análisis topológico de datos, ofreciendo una metodología robusta para visualizar y analizar datos en alta dimensión.\n\nEl algoritmo construye representaciones topológicas de los datos mediante aproximaciones locales del manifold y uniendo estas representaciones en un conjunto simplicial difuso. Minimiza la entropía cruzada entre las representaciones topológicas de los espacios de alta y baja dimensión para lograr una proyección coherente.\n\n Se dará un resumen básico del método, sin embargo se recomienda leer el artículo original de UMAP para una comprensión más profunda.\n\n### Análisis topológico de datos y complejos simpliciales\n\n Geometricamente, un \\(k\\)-simplex es un objeto \\(k\\)-dimensional que es simplemente la envolutra convexa de \\(k+1\\) puntos en un espacio \\(k\\)-dimensional. Un 0-simplex es un vértice, un 1-simplex es una arista, un 2-simplex es un triángulo, un 3-simplex es un tetraedro, etc.\n\n![](https://umap-learn.readthedocs.io/en/latest/_images/simplices.png)\n\nUn complejo simplicial \\(K\\) es una colección de simplexes que cumple con dos propiedades:\n\n1. Cada cara de un simplex en \\(K\\) también está en \\(K\\).\n2. La intersección de dos simplexes en \\(\\sigma_{1}, \\sigma_{2} \\in K\\) es una cara de ambos \\(\\sigma_{1}\\) y \\(\\sigma_{2}\\).\n\n![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy83NDkzMzg1LWViNjY0NDgxNWMxYzk2YTgucG5n?x-oss-process=image/format,png#pic_center)\n\n### Construcción intuitiva de UMAP\n\nUn conjunto de datos es solo una colección finita de puntos en un espacio. En general para entender las características topoloficas, necesitas crear una covertura abierta del espacio. Si los datos están en un espacio métrico, una forma de aproximar esas coverturas abiertas es con bolas abiertas alrededor de cada punto.\n\nPor ejemplo, suponga que se tiene un conjunto con esta forma:\n\n![](https://umap-learn.readthedocs.io/en/latest/_images/how_umap_works_raw_data.png)\n\nSi se toma cada punto y se dibuja una bola alrededor de él, se obtiene algo como esto:\n\n![](https://umap-learn.readthedocs.io/en/latest/_images/how_umap_works_open_cover.png)\n\nPodemos generar un complejo simplicial a través de un complejo Vietoris-Rips. Este complejo se construye tomando cada bola y creando una arista entre cada par de bolas que se superponen. Luego, se crean triángulos entre cada terna de bolas que se superponen, y así sucesivamente.\n\n![](https://umap-learn.readthedocs.io/en/latest/_images/how_umap_works_basic_graph.png)\n\nEsto genera que ahora los datos estén representados a través de un grafo en baja dimensión.\n\n### Adaptación del problema a datos reales\n\n**Problema #1: Escogencia del radio**\n\nLa técnica anterior tiene un problema, no sabemos de antemano el radio óptimo de las bolas. Entonces:\n\n+ Radio es muy pequeno -> No se capturan las relaciones entre los puntos\n+ Radio es muy grande -> Se pierde la estructura local de los datos\n\n**Solución:** Asumir que los datos son uniformes en la variedad \n\n![](https://umap-learn.readthedocs.io/en/latest/_images/how_umap_works_uniform_distribution_cover.png)\n\nEl problema es que este tipo de supuesto no es real para toda la variedad. El problema es que la noción de distancia varía de punto a punto. En algunos puntos es más largo otros más corto. \n\nSin embargo, podemos construir una aproximación de uniformidad local de los puntos usando la geometría Riemaniana. Esto es que la bola alrededor de un punto se extiende hasta los \\(k\\) vecinos más cercanos. Así que cada punto tendrá su propia función de distancia. \n\nDesde un punto topológico, \\(k\\) significa qué tanto queremos estimar la métrica Riemaniana localmente. Si \\(k\\) es pequeño se explicaría features muy locales. Si \\(k\\) es grande, el features sería más global. \n\n![](https://umap-learn.readthedocs.io/en/latest/_images/how_umap_works_local_metric_open_cover.png)\n\n### Un beneficio de la geometría Riemaniana\n\n\nSe puede tener un espacio métrico asociado con cada punto. Es decir, cada punto puede medir distancia de forma significativa de modo que se puede estimar el peso de las aristas del grafo con las distancias que se genera. \n\nAhora, pensemos que si en lugar de decir que la covertura fue una un \"si\" o \"no\", fuera un concepto más difuso como un valor de 0 a 1. Entonces, a partir del cierto punto, el valor se vuelve mas cercano a 0 conforme nos alejamos de este. \n\n![](https://umap-learn.readthedocs.io/en/latest/_images/how_umap_works_fuzzy_open_cover.png)\n \n\n**Problema #2: El manifold podría no estar conectado totalmente.**\n\nEs decir, el manifold podría ser simplemente un montón de islas de puntos sin vecinos muy cercanos. \n\n**Solución:** Usar la conectividad local. \n\nEl algoritmo asume que el manifold es **localmente conexo**. Debido a la maldición de la dimensionalidad, los datos en un espacio de alta dimensión tienen una mayor distancia, pero también pueden ser más similares entre sí. Esto significa que la distancia al primer vecino más cercano puede ser bastante grande, pero la distancia al décimo vecino más cercano suele ser solo ligeramente mayor (relativamente hablando). La restricción de conectividad local asegura que nos centremos en la diferencia de distancia entre los vecinos más cercanos, no en la distancia absoluta (lo que muestra que la diferencia entre vecinos es pequeña).\n\n\n**Problema 3: Incompatibilidad de la métrica local.**\n\nCada punto tiene una métrica local asociada, y desde el punto de vista del punto \\(a\\), la distancia desde el punto a hasta el punto b puede ser 1.5, pero desde el punto de vista del punto \\(b\\), la distancia desde el punto b hasta el punto a podría ser solo 0.6.\n\nBasándonos en la intuición del gráfico, se puede considerar que esto es un borde dirigido con diferentes pesos, como se muestra en la siguiente figura:\n\n![](https://umap-learn.readthedocs.io/en/latest/_images/how_umap_works_raw_graph.png)\n\nCombinar los dos bordes inconsistentes con pesos a y b juntos, entonces deberíamos tener un peso combinado \\(a+b-a\\cdot b\\). La forma de pensar esto es que el peso es en realidad la probabilidad de que exista el borde (1-símplex). Entonces, el peso combinado es la probabilidad de que exista al menos un borde.\n\nSi aplicamos este proceso para fusionar todos los conjuntos simpliciales difusos juntos, terminamos con un solo complejo simplicial difuso, que podemos considerar nuevamente como un gráfico ponderado. En términos de cálculo, simplemente aplicamos la fórmula de combinación de pesos de bordes a todo el gráfico (el peso de los no bordes es 0). Al final, obtenemos algo como esto.\n\n![](https://umap-learn.readthedocs.io/en/latest/_images/how_umap_works_umap_graph.png)\n\nEntonces, asumiendo que ahora tenemos una representación topológica difusa de los datos (hablando matemáticamente, capturará la topología del manifold detrás de los datos), ¿cómo lo convertimos en una representación de baja dimensión?\n\n### Encontrando una representación de baja dimensión\n\nLa representación de baja dimensión debe tener la misma estructura topologica fuzzy de los datos. Tenemos dos problemas acá: 1. Cómo determinar la representación fuzzy en el espacio de baja dimensión y 2. cómo encontrar una buena. \n\nPara 1., básicamente se haraá el mismo proceso pero con un espacio de \\(\\mathbb{R}^2\\) o \\(\\mathbb{R}^3\\). \n\nCon 2., el problema se resuelve calibrando las mismas distancias de la topología difusa en la variedad con respecto a la distancias de la topología en \\(\\mathbb{R}^{2}\\). \n\n\nRecordando el método de procesamiento de peso anterior, interpretamos el peso como la probabilidad de la existencia de un símplex. Dado que las dos topologías que estamos comparando comparten el mismo 0-símplex, es concebible que estamos comparando dos vectores de probabilidad indexados por el 1-símplex. Suponiendo que estos son todas variables de Bernoulli (el símplex final existe o no, y la probabilidad es un parámetro de la distribución de Bernoulli), la elección correcta aquí es la entropía cruzada.\n\nPara entender el proceso priero definamos algunos conceptos. \n\nUsando los \\(k\\) vecinos más cercanos para \\(x_i\\) es el conjunto de puntos \\(\\{x_{i_{1}}, \\dots, x_{i_{k}}\\}\\) tal que: \n\n\\[\n\\rho_{i} = \\min_{1 \\leq j \\leq k} d(x_{i}, x_{i_{j}})\n\\] \n\nLa función de peso para el 1-símplex \\(\\{x_{i}, x_{j}\\}\\) es:\n\n\\[\nw_{h}(x_{i}, x_{j}) = \\exp\\left(-\\dfrac{d(x_{i}, x_{j}) - \\rho_{i}}{\\sigma_{i}}\\right)\n\\]\n \nSi el conjunto de todos los posibles 1-símplexes entre \\(x_i\\) y \\(x_j\\) y la función ponderada hace que \\(w_h(x_{i}, x_{j})\\) sea el peso de ese simplex en la dimensión alta, y $w_l(x_i^{l}, x_j^{l})$ en la dimensión baja, entonces la entropía cruzada es:\n\n\\[\n\\sum_{i=1}^{N} \\sum_{j=1}^{N} w_{h}(x_{i}, x_{j}) \\frac{\\log(w_{h}(x_{i}, x_{j}))}{\\log(w_{l}(x^{l}_{i}, x^{l}_{j}))} + (1-w_{h}(x_{i}, x_{j})) \\frac{\\log(1-w_{h}(x_{i}, x_{j}))}{\\log(1-w_{l}(x^{l}_{i}, x^{l}_{j}))}\n\\]\n\nDesde la perspectiva de los gráficos, minimizar la entropía cruzada se puede considerar como un algoritmo de diseño de gráficos dirigido por fuerza.\n\nEl primer ítem, \\(w_h(e) \\log(w_h(e)/w_l(e))\\) proporciona atracción entre los puntos \\(e\\) cuando hay un peso mayor en el espacio de alta dimensión. Al minimizar este sumando \\(w_l(e)\\) debe ser lo más grande posible y la distancia entre puntos es lo más pequeña posible.\n\nEl segundo sumando, \\((1 - w_h(e)) \\log((1 - w_h(e))/(1 - w_l(e)))\\) proporciona fuerza repulsiva entre los dos segmentos de  \\(e\\) cuando \\(w_h(e)\\) es pequeño. Al hacer \\(w_l(e)\\) lo más pequeño posible, se minimiza esta parte. \n\n\n### Ejemplos \n\n::: {#f9509b8c .cell execution_count=5}\n``` {.python .cell-code}\nimport umap\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_digits\nimport pandas\n\n# Cargar el conjunto de datos\ndigits = load_digits()\ndata = digits.data\ntarget = digits.target\n\n# Instanciar UMAP y reducir la dimensionalidad\nreducer = umap.UMAP(random_state=42)\ndata_reduced = reducer.fit_transform(data)\ndata_reduced = pandas.DataFrame(data_reduced, columns=[\"x\", \"y\"])\n# Visualizar el resultado\n\n\nsns.scatterplot(data= data_reduced,x = \"x\", y=\"y\", hue=target, palette='tab10')\nplt.title('UMAP projection of the Digits dataset')\nplt.show()\n\n# Compare el ressultado con PCA\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\ndata_pca = pca.fit_transform(data)\ndata_pca = pandas.DataFrame(data_pca, columns=[\"x\", \"y\"])\n\nsns.scatterplot(data= data_pca,x = \"x\", y=\"y\", hue=target, palette='tab10')\nplt.title('PCA projection of the Digits dataset')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n2024-02-12 14:56:06.076788: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n/usr/local/lib/python3.11/site-packages/umap/umap_.py:1943: UserWarning:\n\nn_jobs value -1 overridden to 1 by setting random_state. Use no seed for parallelism.\n\nOMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](8_aprendizaje_no_supervisado_files/figure-html/cell-6-output-2.png){width=587 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![](8_aprendizaje_no_supervisado_files/figure-html/cell-6-output-3.png){width=596 height=449}\n:::\n:::\n\n\n::: {#80418bc0 .cell execution_count=6}\n``` {.python .cell-code}\nimport umap.umap_ as umap\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.svm import SVC\n\n# Cargar el conjunto de datos\ndigits = load_digits()\ndata = digits.data\ntarget = digits.target\n\n# Dividir el conjunto de datos\nX_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.25, random_state=42)\n\n```\n:::\n\n\n::: {#2b503e4e .cell execution_count=7}\n``` {.python .cell-code}\n# Reducción de dimensionalidad con UMAP\numap_reducer = umap.UMAP(random_state=42)\nX_train_reduced = umap_reducer.fit_transform(X_train)\nX_test_reduced = umap_reducer.transform(X_test)\n\n# Claificación con SVM\nsvm = SVC()\nsvm.fit(X_train_reduced, y_train)\n\n# Predicción y evaluación\ny_pred = svm.predict(X_test_reduced)\nprint(\"Accuracy con UMAP:\", accuracy_score(y_test, y_pred))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/usr/local/lib/python3.11/site-packages/umap/umap_.py:1943: UserWarning:\n\nn_jobs value -1 overridden to 1 by setting random_state. Use no seed for parallelism.\n\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy con UMAP: 0.9666666666666667\n```\n:::\n:::\n\n\n::: {#e528869c .cell execution_count=8}\n``` {.python .cell-code}\n# Clasificación con SVM\nsvm = SVC()\nsvm.fit(X_train, y_train)\n\n# Predicción y evaluación\ny_pred = svm.predict(X_test)\nprint(\"Accuracy sin UMAP:\", accuracy_score(y_test, y_pred))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy sin UMAP: 0.9866666666666667\n```\n:::\n:::\n\n\n::: {#09f0e86d .cell execution_count=9}\n``` {.python .cell-code}\n# Reducción de dimensionalidad con PCA\npca = PCA(n_components=2)\nX_train_pca = pca.fit_transform(X_train)\nX_test_pca = pca.transform(X_test)\n\n# Clasificación con SVM\nsvm = SVC()\nsvm.fit(X_train_pca, y_train)\n\n# Predicción y evaluación\ny_pred = svm.predict(X_test_pca)\nprint(\"Accuracy con PCA:\", accuracy_score(y_test, y_pred))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy con PCA: 0.6577777777777778\n```\n:::\n:::\n\n\n",
    "supporting": [
      "8_aprendizaje_no_supervisado_files"
    ],
    "filters": [],
    "includes": {}
  }
}
{
  "hash": "c6349d06955eeb1a66e952d8f236bbb1",
  "result": {
    "markdown": "# Algoritmos fundamentales\n\n## Regresión Lineal\n\n## Regresión Logística\n\nLa regresión logística no es una regresión como la regresión lineal, sino, es un método de clasificación. Acá queremos modelar $y_{i}$ como una función lineal de los $x_{i}$.\n\nSe definen las etiquetas negativas como 0 y las positivas como 1 (en el caso de la clasificación binaria). Así, si el valor dado por el modelo es cercano a 0 se le asigna la etiqueta negativa y si es cercano a 1, la positiva. Una función que cumple lo anterior es la **función sigmoide**, la cual está definida como:\n$$\nf(x) = \\dfrac{1}{1+e^{-x}}\n$$\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport math\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nx = np.linspace(-6, 6, 500)\ny = 1 / (1 + np.exp(-x))\nsns.lineplot(x=x, y=y)\nplt.xlabel(\"x\")\nplt.ylabel(\"f(x)\")\nplt.title(\"Función Sigmoide\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](2_algoritmos_fundamentales_files/figure-html/cell-2-output-1.png){width=589 height=450}\n:::\n:::\n\n\nSi se optimizan los valores de $\\mathbb{x}$ y $b$ apropiadamente, podemos interpretar la salida de $f(x)$ como la probabilidad que $y_{i}$ sea positivo. Así, si esta es mayor o igual a $0.5$ podemos decir que la clase de $\\mathbb{x}$ es positiva; en el caso contrario, negativa.\n\nCon base en lo anterior, se define el modelo de regresión logística como:\n$$\nf_{\\mathbb{w}, b}(\\mathbb{x})=\\dfrac{1}{1+e^{-(\\mathbb{w}\\cdot\\mathbb{x}+b)}}\n$$\ndonde el término $\\mathbb{w}\\cdot\\mathbb{x}+b$ es familiar de la regresión lineal.\n\nPara hallar los valores óptimos de la regresión logística, queremos maximizar la verosimilitud basados en los datos de entrenamiento de nuestro modelo. Como estamos en el aprendizaje supervisado, asumimos que tenemos datos etiquetados $(x_i, y_i)$.\n\nEl criterio de optimización en la regresión logística es llamado **máxima verosimilitud**, entonces queremos maximizar:\n$$\nL_{\\mathbb{w}, b} := \\prod_{i=1}^{N}f_{\\mathbb{w}, b}(\\mathbb{x}_{i})^{y_i}(1-f_{\\mathbb{w}, b}(\\mathbb{x}_{i}))^{1-y_i}\n$$\n\nPara maximizar la ecuación anterior, es más sencillo con la *Log*-verosimilitud debido al uso de la función exponencial, la cual se define como:\n$$\n\\log{L_{\\mathbb{w}, b}}\n:= \\ln(L_{\\mathbb{w}, b})\n= \\displaystyle\\sum_{i=1}^{N}y_{i}\\ln(f_{\\mathbb{w}, b}(\\mathbb{x}_{i}))+(1-y_{i})\\ln(1-f_{\\mathbb{w}, b}(\\mathbb{x}_{i}))\n$$\n\nUna forma apropiada en la practica para solucionar el problema de optimizacción es usar el *descenso del gradiente*.\n\n### Ejemplo\n\nLo primero es definir la base de datos:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ncol_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label']\n# load dataset\npima = pd.read_csv(\"datos/diabetes.csv\", header=None, names=col_names)\npima = pima.drop(0)\npima.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pregnant</th>\n      <th>glucose</th>\n      <th>bp</th>\n      <th>skin</th>\n      <th>insulin</th>\n      <th>bmi</th>\n      <th>pedigree</th>\n      <th>age</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>6</td>\n      <td>148</td>\n      <td>72</td>\n      <td>35</td>\n      <td>0</td>\n      <td>33.6</td>\n      <td>0.627</td>\n      <td>50</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>85</td>\n      <td>66</td>\n      <td>29</td>\n      <td>0</td>\n      <td>26.6</td>\n      <td>0.351</td>\n      <td>31</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8</td>\n      <td>183</td>\n      <td>64</td>\n      <td>0</td>\n      <td>0</td>\n      <td>23.3</td>\n      <td>0.672</td>\n      <td>32</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>89</td>\n      <td>66</td>\n      <td>23</td>\n      <td>94</td>\n      <td>28.1</td>\n      <td>0.167</td>\n      <td>21</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>137</td>\n      <td>40</td>\n      <td>35</td>\n      <td>168</td>\n      <td>43.1</td>\n      <td>2.288</td>\n      <td>33</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nA continuación, tomamos las *features* o características y la etiqueta.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfeature_cols = [\"pregnant\", \"insulin\", \"bmi\", \"age\", \"glucose\", \"bp\", \"pedigree\"]\nX = pima[feature_cols]  # Features\ny = pima.label  # Target variable\n```\n:::\n\n\nAhora, dividimos el dataset en set de entrenamiento y prueba.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# split X and y into training and testing sets\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=16\n)\n```\n:::\n\n\nCreamos el objetivo de regresión logística, se entrena y se aplica para predecir los datos de prueba\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# import the class\nfrom sklearn.linear_model import LogisticRegression\n\n# instantiate the model (using the default parameters)\nlogreg = LogisticRegression(random_state=16)\n\n# fit the model with data\nlogreg.fit(X_train, y_train)\n\ny_pred = logreg.predict(X_test)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning:\n\nlbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n\n```\n:::\n:::\n\n\nPara validar que tan bien (o mal) está nuestra predicción podemos verificarlo con una matriz de confunsión:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# import the metrics class\nfrom sklearn import metrics\n\ncnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n\nclass_names = [0, 1]  # name  of classes\nfig, ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\n# create heatmap\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\", fmt=\"g\")\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title(\"Confusion matrix\", y=1.1)\nplt.ylabel(\"Actual label\")\nplt.xlabel(\"Predicted label\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](2_algoritmos_fundamentales_files/figure-html/cell-7-output-1.png){width=644 height=533}\n:::\n:::\n\n\nEsta matriz nos dice que:\n\n* 115 variables fueron asignadas como 0 cuando inicialmente son 0.\n* 10 variables fueron asignadas como 1 cuando inicialmente son 0 (error tipo 1).\n* 24 variables fueron asignadas como 0 cuando inicialmente son 1 (error tipo 2).\n* 43 variables fueron asignadas como 1 cuando inicialmente son 1.\n\n### Ventajas\n\n* No requiere alta potencia computacional debido a la eficiencia y sencillez.\n* No requiere escalar las variables.\n\n### Desventajas\n\n* No es capaz de manejar una gran cantidad de características categóricas.\n* Es vulnerable al *overfitting*.\n* No se puede resolver problemas no lineales con esta regresión, se deben aplicar ciertas transformaciones.\n* No funciona con variables independientes o no correlacionadas con la variable a predecir.\n\n## Decision Trees\n\n### Definición\n\nUn árbol de decisión (DT) es un grafo no cíclico que se utiliza para tomar decisiones (clasificar). En cada nodo (rama) del grafo se evalúa uno de los *features*. Si el resultado de la evaluación es cierto (o está debajo de un umbral), se sigue la rama de la izquierda, si no se va a la derecha.\n\n![](./figuras/DT01.svg){fig-alt=\"Un árbol de decisión.\" fig-align=\"center\"}\n\nPor lo tanto, los DT son un modelo no paramétrico.\n\nPara crear el DT, se **intenta** optimizar el promedio de la máxima verosimilitud:\n$$\n    \\frac{1}{N} \\sum_{i=1}^{N}\\left( y_i \\ln{f_{ID3}(x_i)} + (1-y_i) \\ln{(1-f_{ID3}(x_i))}\\right)\n$$\ndonde $f_{ID3}$ es un DT y $f_{ID3}(x) \\stackrel{\\text{def}}{=} Pr(y=1|x)$\n\n### Construcción\n\nPara construir el árbol, en cada nodo de decisión, se intenta minimizar la entropía de la información.\n\nLa entropía de un conjunto $\\cal{S}$ viene dada por:\n$$\n H(S) \\stackrel{\\text{def}}{=} -f_{ID3}^{S} \\log_2 (f_{ID3}^{S}) - (1-f_{ID3}^{S}) \\log_2 (1-f_{ID3}^{S})\n$$\n\nY si un grupo se divide en dos, la entropía es la suma ponderada de cada subconjunto:\n$$\n  H(S_-, S_+) \\stackrel{\\text{def}}{=} \\frac{|S_-|}{|S|}H(S_-) + \\frac{|S_+|}{|S|}H(S_+)\n$$\n\n### Ejemplo\n\nConsideremos los siguientes datos:\n\nAtributos:\n\n* Edad: viejo (v), media-vida(m), nuevo (nv)\n* Competencia: no(n), sí(s)\n* Tipo: software (swr), hardware (hwr)\n\n| Edad | Competencia | Tipo | Ganancia |\n|------|-------------|------|----------|\n| v    | s           | swr  | baja     |\n| v    | n           | swr  | baja     |\n| v    | n           | hwr  | baja     |\n| m    | s           | swr  | baja     |\n| m    | s           | hwr  | baja     |\n| m    | n           | hwr  | sube     |\n| m    | n           | swr  | sube     |\n| nv   | s           | swr  | sube     |\n| nv   | n           | hwr  | sube     |\n| nv   | n           | swr  | sube     |\n\nCálculo de las entropías: Primero se tiene que probar todos los features para ver cuál tiene mayor ganancia de información (reduce la entropía)\n\nEntropía total:\n$$\nH(S) = \\text{Entropía de los casos baja} + \\text{Entropía de los casos sube}\n$$\n\n$$\nH(s) = -\\frac{5}{10}*\\log_2(\\frac{5}{10}) - \\frac{5}{10}*\\log_2(\\frac{5}{10}) = 1\n$$\n\nAhora vamos a decidir la primera separación con las edades\n![](./figuras/DT02.svg){fig-align=\"center\"}\n$H = \\frac{3}{10} \\cdot 0 + \\frac{4}{10} \\cdot 1 + \\frac{3}{10} \\cdot 0 = 0.4$\n\nAhora vamos a decidir la primera separación con la competencia\n![](./figuras/DT03.svg){fig-align=\"center\"}\n$H = \\frac{4}{10} \\cdot 0.811 + \\frac{6}{10} \\cdot 0.918 = 0.8752$\n\nAhora vamos a decidir la primera separación con las edades\n![](./figuras/DT03.svg){fig-align=\"center\"}\n$H = \\frac{4}{10} \\cdot 0.811 + \\frac{6}{10} \\cdot 0.918 = 0.8752$\n\nAhora vamos a decidir la primera separación con el tipo\n![](./figuras/DT04.svg){fig-align=\"center\"}\n$H = \\frac{6}{10} \\cdot 1 + \\frac{4}{10} \\cdot 1 = 1$\n\nConcluimos que lo que nos da la máxima ganancia de información es primero decidir por edades, eso nos deja dos nodos hoja y un nodo rama que debemos volver a separar.\n\nAhora vamos a buscar el segundo nivel, donde vamos a separar el grupo que tiene edades medias por competencia:\n\n![](./figuras/DT05.svg){fig-align=\"center\"}\n$H = \\frac{2}{4} \\cdot 0 + \\frac{2}{4} \\cdot 0 = 0$\n\nCon esto ya se clasificaron todos los datos, puesto que terminamos solo con nodos hojas:\n![](./figuras/DT06.svg){fig-align=\"center\"}\n\nEsto también se puede hacer con valores numéricos, que de hecho, es lo que se puede hacer con scikit learn\n![](./figuras/DT11.svg){fig-align=\"center\"}\n\ny con esto se obtiene este árbol de decisión:\n![](./figuras/DT12.svg){fig-align=\"center\"}\n\n### Comandos básicos en python\n\nEstos son los comandos básicos en python\n\n```python\n#| label: dibujoArbol01\n#| fig-cap: \"Árbol de decisión\"\nfrom sklearn import tree\nX = # Lista con los features (lista de listas)\nY = # Lista con los labels\n# Se define la variable que tendrá el árbol\nclf = tree.DecisionTreeClassifier()\n# Se calcula el árbol\nclf = clf.fit(X, Y)\n# Se utiliza el árbol para predecir el label de un dato nuevo\nclf.predict_proba(X0)\n# Se dibuja el árbol\ntree.plot_tree(clf)\n```\n\ny este sería un ejemplo sencillo en python:\n\nPrimero creamos los datos\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nfrom sklearn import tree\nfrom sklearn.datasets import make_blobs\nfrom sklearn.inspection import DecisionBoundaryDisplay\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Creación de los datos\nX, Y = make_blobs(n_samples=200, centers=4, random_state=6)\nplt.scatter(X[:, 0], X[:, 1], c=Y, s=30)\nplt.title(\"Datos originales\")\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Ejemplo hecho en python: datos](2_algoritmos_fundamentales_files/figure-html/ejemplo01datos-output-1.png){#ejemplo01datos width=596 height=449}\n:::\n:::\n\n\nLuego se crea el arbol\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(X, Y)\ntree.plot_tree(clf)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Ejemplo hecho en python: arbol](2_algoritmos_fundamentales_files/figure-html/ejemplo01arbol-output-1.png){#ejemplo01arbol width=540 height=389}\n:::\n:::\n\n\ny por último, dibujamos las separaciones\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nDecisionBoundaryDisplay.from_estimator(clf, X, response_method=\"predict\")\nplt.scatter(X[:, 0], X[:, 1], c=Y, s=30)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Ejemplo hecho en python: separación](2_algoritmos_fundamentales_files/figure-html/ejemplo01separacion-output-1.png){#ejemplo01separacion width=577 height=411}\n:::\n:::\n\n\ny con esto se puede aplicar el árbol\n\n::: {#ejemplo01aplicacion .cell execution_count=10}\n``` {.python .cell-code}\nprint(clf.predict([[5.0, 1.0]]))\nprint(clf.predict([[-2.0, -1.0]]))\nprint(clf.predict([[6.0, -6.0]]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0]\n[3]\n[0]\n```\n:::\n:::\n\n\ny lo que devuelve es el número de grupo al que pertene el dato\n\n## Máquinas de Soporte Vectorial\n\nEl problema con SVM es que ocurre si los datos no se pueden separar con un hiperplano.\n\nRecordemos que en SVM, se deben satisfacer las siguientes condiciones:\na. \\(wx_i-b \\geq 1\\) si \\(y_i = 1\\), y\nb. \\(wx_i-b \\leq -1\\) si \\(y_i = -1\\)\n\nY la función a minimizar es:\n\n\\begin{equation*}\n    \\min_{w,b} \\frac{1}{2}||w||^2 \\text{ sujeto a } y_i(wx_i-b) \\geq 1 \\text{ para } i=1,2,\\ldots,N.\n\\end{equation*}\n\n### El truco del kernel\n\nEl truco del kernel consiste en transformar los datos a un espacio de mayor dimensión, donde se pueda separar con un hiperplano. El truco consiste en definir una función \\(\\phi\\) tal que \\(\\phi: x\\to \\phi(x)\\), donde \\(\\phi(x)\\)\n es un vector de dimensión superior.\n\n El problema de minimización se puede reescribir como:\n\n \\begin{equation*}\n\\max_{\\alpha_1, \\dots, \\alpha_N} \\sum_{i=1}^{N} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} y_i \\alpha_i <x_i, x_j> y_j \\alpha_j\n\\end{equation*}\n\nsujeto a que \\(\\alpha_i \\geq 0\\) y \\(\\sum_{i=1}^{N} \\alpha_i y_i = 0\\), donde \\(<x_i, x_j>\\) es el producto punto entre los vectores \\(x_i\\) y \\(x_j\\).\n\nEn este caso los kernels \\(<x_i, x_j>\\) se pueden definir como:\n\n* Lineal: \\(<x_i, x_j> = x_i^T x_j\\)\n* Polinomial: \\(<x_i, x_j> = (\\gamma x_i^T x_j + r)^d\\)\n* Gaussiano o RBF:  \\(<x_i, x_j> = \\exp{(-\\gamma \\Vert x_{i} - x_{j}\\Vert^2)}\\)\n* Sigmoide: \\(<x_i, x_j> = \\tanh{(\\gamma x_i^T x_j + r)}\\)\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nX = np.array(\n    [\n        [0.4, -0.7],\n        [-1.5, -1.0],\n        [-1.4, -0.9],\n        [-1.3, -1.2],\n        [-1.1, -0.2],\n        [-1.2, -0.4],\n        [-0.5, 1.2],\n        [-1.5, 2.1],\n        [1.0, 1.0],\n        [1.3, 0.8],\n        [1.2, 0.5],\n        [0.2, -2.0],\n        [0.5, -2.4],\n        [0.2, -2.3],\n        [0.0, -2.7],\n        [1.3, 2.1],\n    ]\n)\n\ny = np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1])\n\n# Plotting settings\nfig, ax = plt.subplots(figsize=(4, 3))\nx_min, x_max, y_min, y_max = -3, 3, -3, 3\nax.set(xlim=(x_min, x_max), ylim=(y_min, y_max))\n\n# Plot samples by color and add legend\nscatter = ax.scatter(X[:, 0], X[:, 1], s=150, c=y, label=y, edgecolors=\"k\")\nax.legend(*scatter.legend_elements(), loc=\"upper right\", title=\"Classes\")\nax.set_title(\"Samples in two-dimensional feature space\")\n_ = plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](2_algoritmos_fundamentales_files/figure-html/cell-12-output-1.png){width=367 height=283}\n:::\n:::\n\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nfrom sklearn import svm\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\n\ndef plot_training_data_with_decision_boundary(kernel):\n    # Train the SVC\n    clf = svm.SVC(kernel=kernel, gamma=2).fit(X, y)\n\n    # Settings for plotting\n    _, ax = plt.subplots(figsize=(4, 3))\n    x_min, x_max, y_min, y_max = -3, 3, -3, 3\n    ax.set(xlim=(x_min, x_max), ylim=(y_min, y_max))\n\n    # Plot decision boundary and margins\n    common_params = {\"estimator\": clf, \"X\": X, \"ax\": ax}\n    DecisionBoundaryDisplay.from_estimator(\n        **common_params,\n        response_method=\"predict\",\n        plot_method=\"pcolormesh\",\n        alpha=0.3,\n    )\n    DecisionBoundaryDisplay.from_estimator(\n        **common_params,\n        response_method=\"decision_function\",\n        plot_method=\"contour\",\n        levels=[-1, 0, 1],\n        colors=[\"k\", \"k\", \"k\"],\n        linestyles=[\"--\", \"-\", \"--\"],\n    )\n\n    # Plot bigger circles around samples that serve as support vectors\n    ax.scatter(\n        clf.support_vectors_[:, 0],\n        clf.support_vectors_[:, 1],\n        s=250,\n        facecolors=\"none\",\n        edgecolors=\"k\",\n    )\n    # Plot samples by color and add legend\n    ax.scatter(X[:, 0], X[:, 1], c=y, s=150, edgecolors=\"k\")\n    ax.legend(*scatter.legend_elements(), loc=\"upper right\", title=\"Classes\")\n    ax.set_title(f\" Decision boundaries of {kernel} kernel in SVC\")\n\n    _ = plt.show()\n```\n:::\n\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nplot_training_data_with_decision_boundary(\"linear\")\n```\n\n::: {.cell-output .cell-output-display}\n![](2_algoritmos_fundamentales_files/figure-html/cell-14-output-1.png){width=372 height=283}\n:::\n:::\n\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nplot_training_data_with_decision_boundary(\"poly\")\n```\n\n::: {.cell-output .cell-output-display}\n![](2_algoritmos_fundamentales_files/figure-html/cell-15-output-1.png){width=366 height=283}\n:::\n:::\n\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nplot_training_data_with_decision_boundary(\"rbf\")\n```\n\n::: {.cell-output .cell-output-display}\n![](2_algoritmos_fundamentales_files/figure-html/cell-16-output-1.png){width=360 height=283}\n:::\n:::\n\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nplot_training_data_with_decision_boundary(\"sigmoid\")\n```\n\n::: {.cell-output .cell-output-display}\n![](2_algoritmos_fundamentales_files/figure-html/cell-17-output-1.png){width=386 height=283}\n:::\n:::\n\n\n## K-Nearest Neighbors (KNN)\n\n### Carga de paquetes\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import make_blobs\n```\n:::\n\n\n# Cargas datos\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nX, y = make_blobs(n_samples=1000, centers=3, random_state=6)\n```\n:::\n\n\n# Visualizar los datos\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\nsns.scatterplot(x=X[:,0],y=X[:,1], hue=y)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](2_algoritmos_fundamentales_files/figure-html/cell-20-output-1.png){width=590 height=411}\n:::\n:::\n\n\n# Se normaliza y se divide los datos\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)\n\n# Scale the features using StandardScaler\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n```\n:::\n\n\n# Ajuste y evaluación del modelo\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, y_train)\n\n# predecir con el modelo\ny_pred = knn.predict(X_test)\n\n# evaluarlo\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 1.0\n```\n:::\n:::\n\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\nDecisionBoundaryDisplay.from_estimator(knn, X_train)\nsns.scatterplot(x=X[:,0],y=X[:,1], hue=y)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](2_algoritmos_fundamentales_files/figure-html/cell-23-output-1.png){width=569 height=411}\n:::\n:::\n\n\n",
    "supporting": [
      "2_algoritmos_fundamentales_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}
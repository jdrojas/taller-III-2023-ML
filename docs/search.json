[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Taller de Verano: 100 páginas de Machine Learning",
    "section": "",
    "text": "1 Taller de verano",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Taller de verano</span>"
    ]
  },
  {
    "objectID": "1_introduccion.html",
    "href": "1_introduccion.html",
    "title": "2  Cómo funciona el aprendizaje supervisado",
    "section": "",
    "text": "Veremos el caso de las máquinas de soporte vectorial (SVM) para clasificación.\n\nPaso #1: Cargar librerías\n\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn import svm\nfrom sklearn.datasets import make_blobs\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\nfrom scipy.stats import distributions\nfrom numpy import sum\nimport numpy as np\n\n\nPaso #2: Crear datos\n\nSe crean 40 puntos usando la función make_blobs. Esta crea un conjunto de puntos separados en dos grupos.\n\nX, y = make_blobs(n_samples=40, centers=2, random_state=6)\n\n\nPaso #3: Crear el modelo\n\n\nclf = svm.SVC(kernel=\"linear\", C=1000)\n\n\nPaso #4: Entrenar el modelo\n\n\nclf.fit(X, y)\n\nSVC(C=1000, kernel='linear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC(C=1000, kernel='linear')\n\n\n\nPaso #5: Visualizar el modelo\n\n\nplt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)\n\n# plot the decision function\nax = plt.gca()\nDecisionBoundaryDisplay.from_estimator(\n    clf,\n    X,\n    plot_method=\"contour\",\n    colors=\"k\",\n    levels=[-1, 0, 1],\n    alpha=0.5,\n    linestyles=[\"--\", \"-\", \"--\"],\n    ax=ax,\n)\n# plot support vectors\nax.scatter(\n    clf.support_vectors_[:, 0],\n    clf.support_vectors_[:, 1],\n    s=100,\n    linewidth=1,\n    facecolors=\"none\",\n    edgecolors=\"k\",\n)\nplt.show()\n\n\n\n\n\n\n\n\n\nReferencias\n\nhttps://scikit-learn.org/stable/modules/svm.html#\nhttps://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane.html#sphx-glr-auto-examples-svm-plot-separating-hyperplane-py\n\n\n\n3 Estimación de parametros bayesiano\n\nalpha = 10\nbeta = 10\nn = 20\nNsamp = 201  # no of points to sample at\np = np.linspace(0, 1, Nsamp)\ndeltap = 1./(Nsamp-1)  # step size between samples of p\n\nprior = distributions.beta.pdf(p, alpha, beta)\n\nfor i in range(1, 9):\n\n    r = 2**i\n    n = (3.0/2.0)*r\n    like = distributions.binom.pmf(r, n, p)\n    like = like/(deltap*sum(like))  # for plotting convenience only\n    post = distributions.beta.pdf(p, alpha+r, beta+n-r)\n\n    # make the figure\n    plt.figure()\n    plt.plot(p, post, 'k', label='posterior')\n    plt.plot(p, like, 'r', label='likelihood')\n    plt.plot(p, prior, 'b', label='prior')\n    plt.xlabel('p')\n    plt.ylabel('PDF')\n    plt.legend(loc='best')\n    plt.title('r/n={}/{:.0f}'.format(r, n))\n    plt.show()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Cómo funciona el aprendizaje supervisado</span>"
    ]
  },
  {
    "objectID": "2_algoritmos_fundamentales.html",
    "href": "2_algoritmos_fundamentales.html",
    "title": "3  Algoritmos fundamentales",
    "section": "",
    "text": "3.1 Regresión Lineal",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Algoritmos fundamentales</span>"
    ]
  },
  {
    "objectID": "2_algoritmos_fundamentales.html#regresión-logística",
    "href": "2_algoritmos_fundamentales.html#regresión-logística",
    "title": "3  Algoritmos fundamentales",
    "section": "3.2 Regresión Logística",
    "text": "3.2 Regresión Logística\nLa regresión logística no es una regresión como la regresión lineal, sino, es un método de clasificación. Acá queremos modelar \\(y_{i}\\) como una función lineal de los \\(x_{i}\\).\nSe definen las etiquetas negativas como 0 y las positivas como 1 (en el caso de la clasificación binaria). Así, si el valor dado por el modelo es cercano a 0 se le asigna la etiqueta negativa y si es cercano a 1, la positiva. Una función que cumple lo anterior es la función sigmoide, la cual está definida como: \\[\nf(x) = \\dfrac{1}{1+e^{-x}}\n\\]\n\nimport math\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nx = np.linspace(-6, 6, 500)\ny = 1 / (1 + np.exp(-x))\nsns.lineplot(x=x, y=y)\nplt.xlabel(\"x\")\nplt.ylabel(\"f(x)\")\nplt.title(\"Función Sigmoide\")\nplt.show()\n\n\n\n\n\n\n\n\nSi se optimizan los valores de \\(\\mathbb{x}\\) y \\(b\\) apropiadamente, podemos interpretar la salida de \\(f(x)\\) como la probabilidad que \\(y_{i}\\) sea positivo. Así, si esta es mayor o igual a \\(0.5\\) podemos decir que la clase de \\(\\mathbb{x}\\) es positiva; en el caso contrario, negativa.\nCon base en lo anterior, se define el modelo de regresión logística como: \\[\nf_{\\mathbb{w}, b}(\\mathbb{x})=\\dfrac{1}{1+e^{-(\\mathbb{w}\\cdot\\mathbb{x}+b)}}\n\\] donde el término \\(\\mathbb{w}\\cdot\\mathbb{x}+b\\) es familiar de la regresión lineal.\nPara hallar los valores óptimos de la regresión logística, queremos maximizar la verosimilitud basados en los datos de entrenamiento de nuestro modelo. Como estamos en el aprendizaje supervisado, asumimos que tenemos datos etiquetados \\((x_i, y_i)\\).\nEl criterio de optimización en la regresión logística es llamado máxima verosimilitud, entonces queremos maximizar: \\[\nL_{\\mathbb{w}, b} := \\prod_{i=1}^{N}f_{\\mathbb{w}, b}(\\mathbb{x}_{i})^{y_i}(1-f_{\\mathbb{w}, b}(\\mathbb{x}_{i}))^{1-y_i}\n\\]\nPara maximizar la ecuación anterior, es más sencillo con la Log-verosimilitud debido al uso de la función exponencial, la cual se define como: \\[\n\\log{L_{\\mathbb{w}, b}}\n:= \\ln(L_{\\mathbb{w}, b})\n= \\displaystyle\\sum_{i=1}^{N}y_{i}\\ln(f_{\\mathbb{w}, b}(\\mathbb{x}_{i}))+(1-y_{i})\\ln(1-f_{\\mathbb{w}, b}(\\mathbb{x}_{i}))\n\\]\nUna forma apropiada en la practica para solucionar el problema de optimizacción es usar el descenso del gradiente.\n\n3.2.1 Ejemplo\nLo primero es definir la base de datos:\n\ncol_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label']\n# load dataset\npima = pd.read_csv(\"datos/diabetes.csv\", header=None, names=col_names)\npima = pima.drop(0)\npima.head()\n\n\n\n\n\n\n\n\npregnant\nglucose\nbp\nskin\ninsulin\nbmi\npedigree\nage\nlabel\n\n\n\n\n1\n6\n148\n72\n35\n0\n33.6\n0.627\n50\n1\n\n\n2\n1\n85\n66\n29\n0\n26.6\n0.351\n31\n0\n\n\n3\n8\n183\n64\n0\n0\n23.3\n0.672\n32\n1\n\n\n4\n1\n89\n66\n23\n94\n28.1\n0.167\n21\n0\n\n\n5\n0\n137\n40\n35\n168\n43.1\n2.288\n33\n1\n\n\n\n\n\n\n\nA continuación, tomamos las features o características y la etiqueta.\n\nfeature_cols = [\"pregnant\", \"insulin\", \"bmi\", \"age\", \"glucose\", \"bp\", \"pedigree\"]\nX = pima[feature_cols]  # Features\ny = pima.label  # Target variable\n\nAhora, dividimos el dataset en set de entrenamiento y prueba.\n\n# split X and y into training and testing sets\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=16\n)\n\nCreamos el objetivo de regresión logística, se entrena y se aplica para predecir los datos de prueba\n\n# import the class\nfrom sklearn.linear_model import LogisticRegression\n\n# instantiate the model (using the default parameters)\nlogreg = LogisticRegression(random_state=16)\n\n# fit the model with data\nlogreg.fit(X_train, y_train)\n\ny_pred = logreg.predict(X_test)\n\n/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning:\n\nlbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n\n\n\nPara validar que tan bien (o mal) está nuestra predicción podemos verificarlo con una matriz de confunsión:\n\n# import the metrics class\nfrom sklearn import metrics\n\ncnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n\nclass_names = [0, 1]  # name  of classes\nfig, ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\n# create heatmap\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\", fmt=\"g\")\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title(\"Confusion matrix\", y=1.1)\nplt.ylabel(\"Actual label\")\nplt.xlabel(\"Predicted label\")\nplt.show()\n\n\n\n\n\n\n\n\nEsta matriz nos dice que:\n\n115 variables fueron asignadas como 0 cuando inicialmente son 0.\n10 variables fueron asignadas como 1 cuando inicialmente son 0 (error tipo 1).\n24 variables fueron asignadas como 0 cuando inicialmente son 1 (error tipo 2).\n43 variables fueron asignadas como 1 cuando inicialmente son 1.\n\n\n\n3.2.2 Ventajas\n\nNo requiere alta potencia computacional debido a la eficiencia y sencillez.\nNo requiere escalar las variables.\n\n\n\n3.2.3 Desventajas\n\nNo es capaz de manejar una gran cantidad de características categóricas.\nEs vulnerable al overfitting.\nNo se puede resolver problemas no lineales con esta regresión, se deben aplicar ciertas transformaciones.\nNo funciona con variables independientes o no correlacionadas con la variable a predecir.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Algoritmos fundamentales</span>"
    ]
  },
  {
    "objectID": "2_algoritmos_fundamentales.html#decision-trees",
    "href": "2_algoritmos_fundamentales.html#decision-trees",
    "title": "3  Algoritmos fundamentales",
    "section": "3.3 Decision Trees",
    "text": "3.3 Decision Trees\n\n3.3.1 Definición\nUn árbol de decisión (DT) es un grafo no cíclico que se utiliza para tomar decisiones (clasificar). En cada nodo (rama) del grafo se evalúa uno de los features. Si el resultado de la evaluación es cierto (o está debajo de un umbral), se sigue la rama de la izquierda, si no se va a la derecha.\n\n\n\n\n\nPor lo tanto, los DT son un modelo no paramétrico.\nPara crear el DT, se intenta optimizar el promedio de la máxima verosimilitud: \\[\n    \\frac{1}{N} \\sum_{i=1}^{N}\\left( y_i \\ln{f_{ID3}(x_i)} + (1-y_i) \\ln{(1-f_{ID3}(x_i))}\\right)\n\\] donde \\(f_{ID3}\\) es un DT y \\(f_{ID3}(x) \\stackrel{\\text{def}}{=} Pr(y=1|x)\\)\n\n\n3.3.2 Construcción\nPara construir el árbol, en cada nodo de decisión, se intenta minimizar la entropía de la información.\nLa entropía de un conjunto \\(\\cal{S}\\) viene dada por: \\[\nH(S) \\stackrel{\\text{def}}{=} -f_{ID3}^{S} \\log_2 (f_{ID3}^{S}) - (1-f_{ID3}^{S}) \\log_2 (1-f_{ID3}^{S})\n\\]\nY si un grupo se divide en dos, la entropía es la suma ponderada de cada subconjunto: \\[\n  H(S_-, S_+) \\stackrel{\\text{def}}{=} \\frac{|S_-|}{|S|}H(S_-) + \\frac{|S_+|}{|S|}H(S_+)\n\\]\n\n\n3.3.3 Ejemplo\nConsideremos los siguientes datos:\nAtributos:\n\nEdad: viejo (v), media-vida(m), nuevo (nv)\nCompetencia: no(n), sí(s)\nTipo: software (swr), hardware (hwr)\n\n\n\n\nEdad\nCompetencia\nTipo\nGanancia\n\n\n\n\nv\ns\nswr\nbaja\n\n\nv\nn\nswr\nbaja\n\n\nv\nn\nhwr\nbaja\n\n\nm\ns\nswr\nbaja\n\n\nm\ns\nhwr\nbaja\n\n\nm\nn\nhwr\nsube\n\n\nm\nn\nswr\nsube\n\n\nnv\ns\nswr\nsube\n\n\nnv\nn\nhwr\nsube\n\n\nnv\nn\nswr\nsube\n\n\n\nCálculo de las entropías: Primero se tiene que probar todos los features para ver cuál tiene mayor ganancia de información (reduce la entropía)\nEntropía total: \\[\nH(S) = \\text{Entropía de los casos baja} + \\text{Entropía de los casos sube}\n\\]\n\\[\nH(s) = -\\frac{5}{10}*\\log_2(\\frac{5}{10}) - \\frac{5}{10}*\\log_2(\\frac{5}{10}) = 1\n\\]\nAhora vamos a decidir la primera separación con las edades  \\(H = \\frac{3}{10} \\cdot 0 + \\frac{4}{10} \\cdot 1 + \\frac{3}{10} \\cdot 0 = 0.4\\)\nAhora vamos a decidir la primera separación con la competencia  \\(H = \\frac{4}{10} \\cdot 0.811 + \\frac{6}{10} \\cdot 0.918 = 0.8752\\)\nAhora vamos a decidir la primera separación con las edades  \\(H = \\frac{4}{10} \\cdot 0.811 + \\frac{6}{10} \\cdot 0.918 = 0.8752\\)\nAhora vamos a decidir la primera separación con el tipo  \\(H = \\frac{6}{10} \\cdot 1 + \\frac{4}{10} \\cdot 1 = 1\\)\nConcluimos que lo que nos da la máxima ganancia de información es primero decidir por edades, eso nos deja dos nodos hoja y un nodo rama que debemos volver a separar.\nAhora vamos a buscar el segundo nivel, donde vamos a separar el grupo que tiene edades medias por competencia:\n \\(H = \\frac{2}{4} \\cdot 0 + \\frac{2}{4} \\cdot 0 = 0\\)\nCon esto ya se clasificaron todos los datos, puesto que terminamos solo con nodos hojas: \nEsto también se puede hacer con valores numéricos, que de hecho, es lo que se puede hacer con scikit learn \ny con esto se obtiene este árbol de decisión: \n\n\n3.3.4 Comandos básicos en python\nEstos son los comandos básicos en python\n#| label: dibujoArbol01\n#| fig-cap: \"Árbol de decisión\"\nfrom sklearn import tree\nX = # Lista con los features (lista de listas)\nY = # Lista con los labels\n# Se define la variable que tendrá el árbol\nclf = tree.DecisionTreeClassifier()\n# Se calcula el árbol\nclf = clf.fit(X, Y)\n# Se utiliza el árbol para predecir el label de un dato nuevo\nclf.predict_proba(X0)\n# Se dibuja el árbol\ntree.plot_tree(clf)\ny este sería un ejemplo sencillo en python:\nPrimero creamos los datos\n\nfrom sklearn import tree\nfrom sklearn.datasets import make_blobs\nfrom sklearn.inspection import DecisionBoundaryDisplay\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Creación de los datos\nX, Y = make_blobs(n_samples=200, centers=4, random_state=6)\nplt.scatter(X[:, 0], X[:, 1], c=Y, s=30)\nplt.title(\"Datos originales\")\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.show()\n\n\n\n\nEjemplo hecho en python: datos\n\n\n\n\nLuego se crea el arbol\n\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(X, Y)\ntree.plot_tree(clf)\nplt.show()\n\n\n\n\nEjemplo hecho en python: arbol\n\n\n\n\ny por último, dibujamos las separaciones\n\nDecisionBoundaryDisplay.from_estimator(clf, X, response_method=\"predict\")\nplt.scatter(X[:, 0], X[:, 1], c=Y, s=30)\nplt.show()\n\n\n\n\nEjemplo hecho en python: separación\n\n\n\n\ny con esto se puede aplicar el árbol\n\nprint(clf.predict([[5.0, 1.0]]))\nprint(clf.predict([[-2.0, -1.0]]))\nprint(clf.predict([[6.0, -6.0]]))\n\n[0]\n[3]\n[0]\n\n\ny lo que devuelve es el número de grupo al que pertene el dato",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Algoritmos fundamentales</span>"
    ]
  },
  {
    "objectID": "2_algoritmos_fundamentales.html#máquinas-de-soporte-vectorial",
    "href": "2_algoritmos_fundamentales.html#máquinas-de-soporte-vectorial",
    "title": "3  Algoritmos fundamentales",
    "section": "3.4 Máquinas de Soporte Vectorial",
    "text": "3.4 Máquinas de Soporte Vectorial\nEl problema con SVM es que ocurre si los datos no se pueden separar con un hiperplano.\nRecordemos que en SVM, se deben satisfacer las siguientes condiciones: a. \\(wx_i-b \\geq 1\\) si \\(y_i = 1\\), y b. \\(wx_i-b \\leq -1\\) si \\(y_i = -1\\)\nY la función a minimizar es:\n\\[\\begin{equation*}\n    \\min_{w,b} \\frac{1}{2}||w||^2 \\text{ sujeto a } y_i(wx_i-b) \\geq 1 \\text{ para } i=1,2,\\ldots,N.\n\\end{equation*}\\]\n\n3.4.1 El truco del kernel\nEl truco del kernel consiste en transformar los datos a un espacio de mayor dimensión, donde se pueda separar con un hiperplano. El truco consiste en definir una función \\(\\phi\\) tal que \\(\\phi: x\\to \\phi(x)\\), donde \\(\\phi(x)\\) es un vector de dimensión superior.\nEl problema de minimización se puede reescribir como:\n\\[\\begin{equation*}\n\\max_{\\alpha_1, \\dots, \\alpha_N} \\sum_{i=1}^{N} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} y_i \\alpha_i &lt;x_i, x_j&gt; y_j \\alpha_j\n\\end{equation*}\\]\nsujeto a que \\(\\alpha_i \\geq 0\\) y \\(\\sum_{i=1}^{N} \\alpha_i y_i = 0\\), donde \\(&lt;x_i, x_j&gt;\\) es el producto punto entre los vectores \\(x_i\\) y \\(x_j\\).\nEn este caso los kernels \\(&lt;x_i, x_j&gt;\\) se pueden definir como:\n\nLineal: \\(&lt;x_i, x_j&gt; = x_i^T x_j\\)\nPolinomial: \\(&lt;x_i, x_j&gt; = (\\gamma x_i^T x_j + r)^d\\)\nGaussiano o RBF: \\(&lt;x_i, x_j&gt; = \\exp{(-\\gamma \\Vert x_{i} - x_{j}\\Vert^2)}\\)\nSigmoide: \\(&lt;x_i, x_j&gt; = \\tanh{(\\gamma x_i^T x_j + r)}\\)\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nX = np.array(\n    [\n        [0.4, -0.7],\n        [-1.5, -1.0],\n        [-1.4, -0.9],\n        [-1.3, -1.2],\n        [-1.1, -0.2],\n        [-1.2, -0.4],\n        [-0.5, 1.2],\n        [-1.5, 2.1],\n        [1.0, 1.0],\n        [1.3, 0.8],\n        [1.2, 0.5],\n        [0.2, -2.0],\n        [0.5, -2.4],\n        [0.2, -2.3],\n        [0.0, -2.7],\n        [1.3, 2.1],\n    ]\n)\n\ny = np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1])\n\n# Plotting settings\nfig, ax = plt.subplots(figsize=(4, 3))\nx_min, x_max, y_min, y_max = -3, 3, -3, 3\nax.set(xlim=(x_min, x_max), ylim=(y_min, y_max))\n\n# Plot samples by color and add legend\nscatter = ax.scatter(X[:, 0], X[:, 1], s=150, c=y, label=y, edgecolors=\"k\")\nax.legend(*scatter.legend_elements(), loc=\"upper right\", title=\"Classes\")\nax.set_title(\"Samples in two-dimensional feature space\")\n_ = plt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn import svm\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\n\ndef plot_training_data_with_decision_boundary(kernel):\n    # Train the SVC\n    clf = svm.SVC(kernel=kernel, gamma=2).fit(X, y)\n\n    # Settings for plotting\n    _, ax = plt.subplots(figsize=(4, 3))\n    x_min, x_max, y_min, y_max = -3, 3, -3, 3\n    ax.set(xlim=(x_min, x_max), ylim=(y_min, y_max))\n\n    # Plot decision boundary and margins\n    common_params = {\"estimator\": clf, \"X\": X, \"ax\": ax}\n    DecisionBoundaryDisplay.from_estimator(\n        **common_params,\n        response_method=\"predict\",\n        plot_method=\"pcolormesh\",\n        alpha=0.3,\n    )\n    DecisionBoundaryDisplay.from_estimator(\n        **common_params,\n        response_method=\"decision_function\",\n        plot_method=\"contour\",\n        levels=[-1, 0, 1],\n        colors=[\"k\", \"k\", \"k\"],\n        linestyles=[\"--\", \"-\", \"--\"],\n    )\n\n    # Plot bigger circles around samples that serve as support vectors\n    ax.scatter(\n        clf.support_vectors_[:, 0],\n        clf.support_vectors_[:, 1],\n        s=250,\n        facecolors=\"none\",\n        edgecolors=\"k\",\n    )\n    # Plot samples by color and add legend\n    ax.scatter(X[:, 0], X[:, 1], c=y, s=150, edgecolors=\"k\")\n    ax.legend(*scatter.legend_elements(), loc=\"upper right\", title=\"Classes\")\n    ax.set_title(f\" Decision boundaries of {kernel} kernel in SVC\")\n\n    _ = plt.show()\n\n\nplot_training_data_with_decision_boundary(\"linear\")\n\n\n\n\n\n\n\n\n\nplot_training_data_with_decision_boundary(\"poly\")\n\n\n\n\n\n\n\n\n\nplot_training_data_with_decision_boundary(\"rbf\")\n\n\n\n\n\n\n\n\n\nplot_training_data_with_decision_boundary(\"sigmoid\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Algoritmos fundamentales</span>"
    ]
  },
  {
    "objectID": "2_algoritmos_fundamentales.html#k-nearest-neighbors-knn",
    "href": "2_algoritmos_fundamentales.html#k-nearest-neighbors-knn",
    "title": "3  Algoritmos fundamentales",
    "section": "3.5 K-Nearest Neighbors (KNN)",
    "text": "3.5 K-Nearest Neighbors (KNN)\n\n3.5.1 Carga de paquetes\n\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import make_blobs",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Algoritmos fundamentales</span>"
    ]
  },
  {
    "objectID": "3_descenso_del_gradiente.html",
    "href": "3_descenso_del_gradiente.html",
    "title": "4  Anatomía de un algoritmo de aprendizaje",
    "section": "",
    "text": "4.1 Bloques de construcción de un algoritmo de aprendizaje\nEn los algoritmos de aprendizaje que abarcamos en la sección anterior podemos observar los 3 bloques que se utilizan para construirlos:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Anatomía de un algoritmo de aprendizaje</span>"
    ]
  },
  {
    "objectID": "3_descenso_del_gradiente.html#bloques-de-construcción-de-un-algoritmo-de-aprendizaje",
    "href": "3_descenso_del_gradiente.html#bloques-de-construcción-de-un-algoritmo-de-aprendizaje",
    "title": "4  Anatomía de un algoritmo de aprendizaje",
    "section": "",
    "text": "Bloques de un algoritmo de aprendizaje.\n\n\n\n\n\n\nBloque\nDescripción\n\n\n\n\nFunción de pérdida\n\nMétodo para evaluar que tan bien se ajusta el modelo a los datos de entrenamiento\nSi el modelo no predice adecuadamente, la función de pérdida da un resultado mayor\n\n\n\nCriterio de optimización\n\nDebe estar basado en la función de pérdida\n\n\n\nRutina de optimización\n\nUtiliza los valores de la función objetivo y los datos para ajustar los parámetros del modelo",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Anatomía de un algoritmo de aprendizaje</span>"
    ]
  },
  {
    "objectID": "3_descenso_del_gradiente.html#descenso-de-gradiente-gd",
    "href": "3_descenso_del_gradiente.html#descenso-de-gradiente-gd",
    "title": "4  Anatomía de un algoritmo de aprendizaje",
    "section": "4.2 Descenso de gradiente (GD)",
    "text": "4.2 Descenso de gradiente (GD)\nEl descenso de gradiente es un algoritmo iterativo que permite encontrar el máximo o mínimo de una función dada, y se utiliza en los algoritmos de machine learning (ML) y deep learning (DL) para minimizar las funiciones de pérdida.\nEn conjunto con el descenso de gradiente estocástico, son de los algoritmos más utilizados en ML y DL.\n\n4.2.1 Requisitos de la función a optimizar\n\nDiferenciable: tiene una derivada para cada punto en su dominio.\n\n\n\n\nFunciones diferenciables\n\n\n\n\n\nFunciones no diferenciables\n\n\n\nConvexa: para una función univariada, una línea que conecta dos puntos de la función pasa sobre o encima de la función. Las funciones convexas solo tienen un mínimo, que es el mínimo global.\n\n\n\n\nFunciones convexas\n\n\nLos criterios de optimización de muchos modelos (regresión lineal y logística, SVM, entre otros) son convexos, por lo que el GD es un método adecuado.\nLos criterios de optimización para las redes neuronales no son convexos (tienen mínimos locales y globales), pero en la práctica es suficiente encontrar mínimos globales, por lo que el GD también resulta un método útil.\n\n\n4.2.2 Gradiente\nEs la pendiente de una curva en una dirección específica.\nEn funciones univariadas la obtenemos evaluando la primera derivada en un punto de interés.\nEn funciones multivariadas, es un vector de derivadas en cada dirección principal, lo que conocemos como derivadas parciales.\nEl gradiente para una función \\(f(x)\\) en un punto \\(p\\) está dado por: \\[\n\\nabla f(p) = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} (p) \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n} (p) \\end{bmatrix}\n\\]\n\n\n4.2.3 Algoritmo de descenso de gradiente\nEl método se puede resumir mediante la siguiente ecuación:\n\\[\np_{n+1} = p_n - \\alpha \\nabla f(p_n)\n\\]\nPaso a paso:\n\nEscoger un punto inicial: \\(p_n\\)\nCalcular el gradiente en este punto: \\(\\nabla f(p_n)\\)\nMoverse en la dirección contraria al gradiente, a una distancia dada por la tasa de aprendizaje \\(\\alpha\\)\nRepetir pasos 2 y 3 hasta que se cumpla lo siguiente:\n\n\nNúmero máximo de iteraciones alcanzado\nEl tamaño del paso es más pequeño que la tolerancia definida (cambio en \\(\\alpha\\) o gradiente muy baja)\n\n\n\n4.2.4 Imports\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom typing import Callable\n\n\n\n4.2.5 Ejemplo 1: función univariada, derivable no convexa\nFunción a optimizar: \\[\\begin{equation*}\nf(x) = x^4 - 2x^3 + 2\n\\end{equation*}\\] Y su gradiente:\n\\[\\begin{equation*}\n\\frac{df(x)}{dx} = 4x^3-6x^2\n\\end{equation*}\\]\nDefiniendo \\(f(x)\\) y \\(\\frac{df(x)}{dx}\\) en python\n\n# f(x)\ndef f_ej1(x:float):\n  return x**4-2*x**3+2\n\n# df(x)/dx\ndef dfdx_ej1(x:float):\n  return 4*x**3-6*x**2\n\n\n\n4.2.6 Gráfica de \\(f(x)\\)\n\nx = np.linspace(-0.8, 2.2, 100)\ny = f_ej1(x)\nplt.grid(True)\n\nplt.plot(x,y)\n\n\n\n\n\n\n\n\nDefiniendo algoritmo gradient_descent\n\ndef gradient_descent(start: float, gradient: Callable[[float], float],\n                     learn_rate: float, max_iter: int, tol: float = 0.01):\n    x = start\n    steps = [start]  # history tracking\n\n    for _ in range(max_iter):\n        diff = learn_rate*gradient(x)\n        if np.abs(diff) &lt; tol:\n            break\n        x = x - diff\n        steps.append(x)  # history tracing\n  \n    return steps, x\n\nLlamando a gradient_descent para el ejemplo 1\n\nstart = 2\ngradient = dfdx_ej1\nlearn_rate = 0.1\nmax_iter = 100\n\ngradient_descent(start, gradient, learn_rate, max_iter)\n\n([2, 1.2, 1.3728000000000002, 1.4686874222592001, 1.4957044497076786],\n 1.4957044497076786)\n\n\n\n\n\n\n\n\nFigura 4.1: Animación del ejemplo 1.\n\n\n\nVariaciones al ejemplo 1\n\n\n\n\n\n\nFigura 4.2: Animación del ejemplo 1 (variación 1).\n\n\n\n\n\n\n\n\n\nFigura 4.3: Animación del ejemplo 1 (variación 2).\n\n\n\n\n\n\n\n\n\nFigura 4.4: Animación del ejemplo 1 (variación 3).\n\n\n\n\n\n4.2.7 Códigos usados para esta sección\n\n# Gradient Descent\n# gcorazzari\n\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport numpy as np\n\nfrom typing import Callable\n\n\ndef update(frame):\n    x = x_gd[:frame]\n    y = y_gd[:frame]\n    line_gd.set_xdata(x)\n    line_gd.set_ydata(y)\n    return line_gd\n\n\ndef gradient_descent(\n    start: float,\n    gradient: Callable[[float], float],\n    learn_rate: float,\n    max_iter: int,\n    tol: float = 0.01,\n):\n    x = start\n    steps = [start]  # history tracking\n\n    for _ in range(max_iter):\n        diff = learn_rate * gradient(x)\n        if np.abs(diff) &lt; tol:\n            break\n        x = x - diff\n        steps.append(x)  # history tracing\n\n    return steps, x\n\n\n# EJ1-4\ndef f_ej1(x: float):\n    return x**4 - 2 * x**3 + 2\n\n\ndef dfdx_ej1(x: float):\n    return 4 * x**3 - 6 * x**2\n\n\nstart = -0.5\ngradient = dfdx_ej1\nlearn_rate = 0.3\nmax_iter = 100\n\nres_ej1 = gradient_descent(start, gradient, learn_rate, max_iter)\n\nfig, ax = plt.subplots()\n\nx = np.linspace(-0.8, 2.2, 100)\ny = f_ej1(x)\n\nline = ax.plot(x, y)\n\nx_gd = np.array(res_ej1[0])\ny_gd = f_ej1(x_gd)\n\nline_gd = ax.plot(\n    res_ej1[0][0], f_ej1(res_ej1[0][0]), \"ro-\", linewidth=0.5, markersize=2\n)[0]\n\nani = animation.FuncAnimation(fig=fig, func=update, frames=102, interval=200)\n\nplt.grid(True)\nplt.title(\"Inicio=-0.5, alpha=0.3\")\n\nwriter = animation.PillowWriter(fps=5)\n\nplt.show()\nani.save(\"ej1-4.gif\", writer=writer)\n\n\n# EJ1-3\ndef f_ej1(x: float):\n    return x**4 - 2 * x**3 + 2\n\n\ndef dfdx_ej1(x: float):\n    return 4 * x**3 - 6 * x**2\n\n\nstart = -0.5\ngradient = dfdx_ej1\nlearn_rate = 0.1\nmax_iter = 100\n\nres_ej1 = gradient_descent(start, gradient, learn_rate, max_iter)\n\nfig, ax = plt.subplots()\n\nx = np.linspace(-0.8, 2.2, 100)\ny = f_ej1(x)\n\nline = ax.plot(x, y)\n\nx_gd = np.array(res_ej1[0])\ny_gd = f_ej1(x_gd)\n\nline_gd = ax.plot(\n    res_ej1[0][0], f_ej1(res_ej1[0][0]), \"ro-\", linewidth=0.5, markersize=2\n)[0]\n\nani = animation.FuncAnimation(fig=fig, func=update, frames=9, interval=500)\n\nplt.grid(True)\nplt.title(\"Inicio=-0.5, alpha=0.1\")\n\nwriter = animation.PillowWriter(fps=5)\n\nplt.show()\nani.save(\"ej1-3.gif\", writer=writer)\n\n\n# EJ1-2\ndef f_ej1(x: float):\n    return x**4 - 2 * x**3 + 2\n\n\ndef dfdx_ej1(x: float):\n    return 4 * x**3 - 6 * x**2\n\n\nstart = 2\ngradient = dfdx_ej1\nlearn_rate = 0.3\nmax_iter = 100\n\nres_ej1 = gradient_descent(start, gradient, learn_rate, max_iter)\n\nfig, ax = plt.subplots()\n\nx = np.linspace(-0.8, 2.2, 100)\ny = f_ej1(x)\n\nline = ax.plot(x, y)\n\nx_gd = np.array(res_ej1[0])\ny_gd = f_ej1(x_gd)\n\nline_gd = ax.plot(\n    res_ej1[0][0], f_ej1(res_ej1[0][0]), \"ro-\", linewidth=0.5, markersize=2\n)[0]\n\nani = animation.FuncAnimation(fig=fig, func=update, frames=4, interval=200)\n\nplt.grid(True)\nplt.title(\"Inicio=2, alpha=0.3\")\n\nwriter = animation.PillowWriter(fps=5)\n\nplt.show()\nani.save(\"ej1-2.gif\", writer=writer)\n\n\n# EJ1-1\ndef f_ej1(x: float):\n    return x**4 - 2 * x**3 + 2\n\n\ndef dfdx_ej1(x: float):\n    return 4 * x**3 - 6 * x**2\n\n\nstart = 2\ngradient = dfdx_ej1\nlearn_rate = 0.1\nmax_iter = 100\n\nres_ej1 = gradient_descent(start, gradient, learn_rate, max_iter)\n\nfig, ax = plt.subplots()\n\nx = np.linspace(-0.8, 2.2, 100)\ny = f_ej1(x)\n\nline = ax.plot(x, y)\n\nx_gd = np.array(res_ej1[0])\ny_gd = f_ej1(x_gd)\n\nline_gd = ax.plot(\n    res_ej1[0][0], f_ej1(res_ej1[0][0]), \"ro-\", linewidth=0.5, markersize=2\n)[0]\n\nani = animation.FuncAnimation(fig=fig, func=update, frames=6, interval=500)\n\nplt.grid(True)\nplt.title(\"Inicio=2, alpha=0.1\")\n\nwriter = animation.PillowWriter(fps=5)\n\nplt.show()\nani.save(\"ej1-1.gif\", writer=writer)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Anatomía de un algoritmo de aprendizaje</span>"
    ]
  },
  {
    "objectID": "4_practica_basica.html",
    "href": "4_practica_basica.html",
    "title": "5  Capítulo 5",
    "section": "",
    "text": "5.1 5.2:Selección de algoritmos\nElegir un algoritmo puede ser una tarea difícil. En el caso de que se disponga de mucho tiempo se pueden probar varios, sin embargo no siempre es el caso, por lo que hay una serie de preguntas que se pueden realizar con el fin de hacer el proceso más eficiente.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Capítulo 5</span>"
    ]
  },
  {
    "objectID": "4_practica_basica.html#selección-de-algoritmos",
    "href": "4_practica_basica.html#selección-de-algoritmos",
    "title": "5  Capítulo 5",
    "section": "",
    "text": "5.1.1 Explicabilidad o interpretabilidad\nCon que facilidad el algoritmo logra explicar las predicciones que realiza, por lo general un modelo que realiza una predicción específica es dificíl de entender y aún más de explicar. Algunos ejemplos son los modelos de redes neuronales o el método de emsamble. Estos algoritmos que carecen de tal explicación se llaman algoritmo de caja negra.Por otro lado, los algoritmos de aprendizaje kNN, regresión lineal o árbol de decisión producen modelos que no siempre son los más precisos, sin embargo, la forma en que hacen su predicción es muy sencilla.\nEjemplo\n\n\n\n\nFigura 1. Método de emsamble\n\n\n\n\n\n5.1.2 Requisitos de memoria\nSi el conjunto de datos se puede cargar de forma completa en la memoria RAM del servidor o computador, entonces la disponibilidad de logaritmos es amplia. Sin embargo, si ese no es el caso, se debe optar por logaritmos de aprendizaje incremental, estos pueden mejorar el modelo añadiendo más datos gradualmente, básicamente se adaptan a nuevos nuevos sin el olvidar la información ya existente.\n\n\n5.1.3 Número de funciones y característica\nAlgunos algoritmos, como las redes neuronales y el descenso de gradiente, pueden manejar un gran número de ejemplos y millones de características. Otros, como SVM, pueden ser muy modestos en su capacidad. Entonces, a la hora de escoger un logaritmo se debe considerar el tamaño de los datos y la cantidad de funciones.\n\n\n5.1.4 Características categóricas frente a numéricas\nAlgunos algoritmos solo pueden funcionar con datos numéricos, por lo que si se tienen datos en un formato categórico o no numérico, se deberá considerar un proceso para convertirlos en datos numéricos mediante técnicas como la codificación one-hot.\n\n\n5.1.5 linealidad de los datos\nSi los datos son linealmente separables o pueden modelarse mediante un modelo lineal, se puede utilizar SVM, regresión logística o la regresión lineal, si no es el caso las redes neuronales o los algoritmos de conjunto, son una mejor opción.\nEjemplo\n\n\n\n\n5.1.6 Velocidad de entrenamiento\nEs el tiempo que tarda un algoritmo en aprender y crear un modelo. Las redes neuronales son conocidas por la considerable cantidad de tiempo que requieren para entrenar un modelo. Los algoritmos de máquina tradicionales como K-Vecinos más cercanos y Regresión logística toman mucho menos tiempo. Algunos algoritmos, como Bosque aleatorio, requieren diferentes tiempos de entrenamiento según los núcleos de CPU que se utilizan.\nEjemplo\n\n\n\n5.1.7 Velocidad de predicción\nTiempo que le toma a un modelo hacer sus predicciones, en este caso se debe considerar qué tan rápido debe ser el modelo a la hora de generar predicciones y para que función se está utilizando el modelo escogido. Si no se quiere adivinar cuál es el mejor algoritmo para los datos, una forma de elegir es utilizar la prueba de validación.\nEjemplo\n\n\n\nFigura 6. Diagrama Selección del algoritmo",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Capítulo 5</span>"
    ]
  },
  {
    "objectID": "4_practica_basica.html#three-sets",
    "href": "4_practica_basica.html#three-sets",
    "title": "5  Capítulo 5",
    "section": "5.2 5.3:Three sets",
    "text": "5.2 5.3:Three sets\nEn Machine Learning, para el estudio y construcción de algoritmos que sean capaces de aprender de los datos y hacer predicciones utilizando esa información, se utiliza la construcción de un modelo matemático a partir de datos de entrada y estos datos para ser utilizados se dividen en conjuntos de datos, estos son:\n\nConjunto de entrenamiento\nConjunto de validación\nConjunto de prueba\n\nEl conjunto de entrenamiento, suele ser el conjunto más grande y es el que se utiliza para construir el modelo, los conjuntos de validación y prueba suelen tener el mismo tamaño y esto son menores que en el conjunto de entrenamiento , tanto los conjuntos de validación y prueba no son usados para construir el modelo. A estos conjuntos se les suele llamar conjuntos esfera.\nEjemplo\n\n\n\n\nFigura 8. Solución problemas",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Capítulo 5</span>"
    ]
  },
  {
    "objectID": "4_practica_basica.html#validación-cruzada",
    "href": "4_practica_basica.html#validación-cruzada",
    "title": "5  Capítulo 5",
    "section": "6.1 Validación Cruzada",
    "text": "6.1 Validación Cruzada\nCuando no se tienen muchos datos para ajustar los hiperparámetros. Entonces, podemos dividir el conjunto de entrenamiento en varios subconjuntos (fold) del mismo tamaño, lo usual es usar 5 folds. Así, se dividen los datos de entrenamiento en 5 folds \\(\\{F_1, F_2, F_3, F_4, F_5\\}\\) cada una contiene el \\(20\\%\\) de los datos de entrenamiento. Así, se entrenan 5 modelos, de la siguiente forma: para el primer modelo \\(f_1\\) se utilizan los folds \\(F_2, F_3, F_4, F_5\\) y \\(F_1\\) se utiliza como conjunto de validación; para el segundo modelo \\(f_2\\) se utilizan los folds \\(F_1, F_3, F_4, F_5\\) y el \\(F_2\\) es el conjutno de validación; y aspi hasta completar \\(f_5\\).\nSe puede aplicar grid search con la validación cruzada para encontrar los mejores valores para los hiperparámetro de nuestro modelo.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Capítulo 5</span>"
    ]
  },
  {
    "objectID": "4_practica_basica.html#ejemplo",
    "href": "4_practica_basica.html#ejemplo",
    "title": "5  Capítulo 5",
    "section": "6.2 Ejemplo",
    "text": "6.2 Ejemplo\n\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\ndigits = datasets.load_digits()\nn_samples = len(digits.images)\nX = digits.images.reshape((n_samples, -1))\ny = digits.target == 8\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n\n\nimport pandas as pd\n\n\ndef print_dataframe(filtered_cv_results):\n    \"\"\"Pretty print for filtered dataframe\"\"\"\n    for mean_precision, std_precision, mean_recall, std_recall, params in zip(\n        filtered_cv_results[\"mean_test_precision\"],\n        filtered_cv_results[\"std_test_precision\"],\n        filtered_cv_results[\"mean_test_recall\"],\n        filtered_cv_results[\"std_test_recall\"],\n        filtered_cv_results[\"params\"],\n    ):\n        print(\n            f\"precision: {mean_precision:0.3f} (±{std_precision:0.03f}),\"\n            f\" recall: {mean_recall:0.3f} (±{std_recall:0.03f}),\"\n            f\" for {params}\"\n        )\n    print()\n\n\ndef refit_strategy(cv_results):\n    \"\"\"Define the strategy to select the best estimator.\n\n    The strategy defined here is to filter-out all results below a precision threshold\n    of 0.98, rank the remaining by recall and keep all models with one standard\n    deviation of the best by recall. Once these models are selected, we can select the\n    fastest model to predict.\n\n    Parameters\n    ----------\n    cv_results : dict of numpy (masked) ndarrays\n        CV results as returned by the `GridSearchCV`.\n\n    Returns\n    -------\n    best_index : int\n        The index of the best estimator as it appears in `cv_results`.\n    \"\"\"\n    # print the info about the grid-search for the different scores\n    precision_threshold = 0.98\n\n    cv_results_ = pd.DataFrame(cv_results)\n    print(\"All grid-search results:\")\n    print_dataframe(cv_results_)\n\n    # Filter-out all results below the threshold\n    high_precision_cv_results = cv_results_[\n        cv_results_[\"mean_test_precision\"] &gt; precision_threshold\n    ]\n\n    print(f\"Models with a precision higher than {precision_threshold}:\")\n    print_dataframe(high_precision_cv_results)\n\n    high_precision_cv_results = high_precision_cv_results[\n        [\n            \"mean_score_time\",\n            \"mean_test_recall\",\n            \"std_test_recall\",\n            \"mean_test_precision\",\n            \"std_test_precision\",\n            \"rank_test_recall\",\n            \"rank_test_precision\",\n            \"params\",\n        ]\n    ]\n\n    # Select the most performant models in terms of recall\n    # (within 1 sigma from the best)\n    best_recall_std = high_precision_cv_results[\"mean_test_recall\"].std()\n    best_recall = high_precision_cv_results[\"mean_test_recall\"].max()\n    best_recall_threshold = best_recall - best_recall_std\n\n    high_recall_cv_results = high_precision_cv_results[\n        high_precision_cv_results[\"mean_test_recall\"] &gt; best_recall_threshold\n    ]\n    print(\n        \"Out of the previously selected high precision models, we keep all the\\n\"\n        \"the models within one standard deviation of the highest recall model:\"\n    )\n    print_dataframe(high_recall_cv_results)\n\n    # From the best candidates, select the fastest model to predict\n    fastest_top_recall_high_precision_index = high_recall_cv_results[\n        \"mean_score_time\"\n    ].idxmin()\n\n    print(\n        \"\\nThe selected final model is the fastest to predict out of the previously\\n\"\n        \"selected subset of best models based on precision and recall.\\n\"\n        \"Its scoring time is:\\n\\n\"\n        f\"{high_recall_cv_results.loc[fastest_top_recall_high_precision_index]}\"\n    )\n\n    return fastest_top_recall_high_precision_index\n\nAhora, se define el grid search\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nscores = ['precision', 'recall']\n\ntuned_params = [\n    {'kernel':['rbf'], 'gamma':[1e-3, 1e-4], 'C':[1, 10, 100, 1000]},\n    {'kernel':['linear'], 'C':[1, 10, 100, 1000]}\n]\n\ngrid_search = GridSearchCV(SVC(), tuned_params, scoring=scores, refit=refit_strategy)\ngrid_search.fit(X_train, y_train)\n\nAll grid-search results:\nprecision: 1.000 (±0.000), recall: 0.854 (±0.063), for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\nprecision: 1.000 (±0.000), recall: 0.257 (±0.061), for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\nprecision: 1.000 (±0.000), recall: 0.877 (±0.069), for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\nprecision: 0.968 (±0.039), recall: 0.780 (±0.083), for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\nprecision: 1.000 (±0.000), recall: 0.877 (±0.069), for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\nprecision: 0.905 (±0.058), recall: 0.889 (±0.074), for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\nprecision: 1.000 (±0.000), recall: 0.877 (±0.069), for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\nprecision: 0.904 (±0.058), recall: 0.890 (±0.073), for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\nprecision: 0.695 (±0.073), recall: 0.743 (±0.065), for {'C': 1, 'kernel': 'linear'}\nprecision: 0.643 (±0.066), recall: 0.757 (±0.066), for {'C': 10, 'kernel': 'linear'}\nprecision: 0.611 (±0.028), recall: 0.744 (±0.044), for {'C': 100, 'kernel': 'linear'}\nprecision: 0.618 (±0.039), recall: 0.744 (±0.044), for {'C': 1000, 'kernel': 'linear'}\n\nModels with a precision higher than 0.98:\nprecision: 1.000 (±0.000), recall: 0.854 (±0.063), for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\nprecision: 1.000 (±0.000), recall: 0.257 (±0.061), for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\nprecision: 1.000 (±0.000), recall: 0.877 (±0.069), for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\nprecision: 1.000 (±0.000), recall: 0.877 (±0.069), for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\nprecision: 1.000 (±0.000), recall: 0.877 (±0.069), for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n\nOut of the previously selected high precision models, we keep all the\nthe models within one standard deviation of the highest recall model:\nprecision: 1.000 (±0.000), recall: 0.854 (±0.063), for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\nprecision: 1.000 (±0.000), recall: 0.877 (±0.069), for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\nprecision: 1.000 (±0.000), recall: 0.877 (±0.069), for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\nprecision: 1.000 (±0.000), recall: 0.877 (±0.069), for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n\n\nThe selected final model is the fastest to predict out of the previously\nselected subset of best models based on precision and recall.\nIts scoring time is:\n\nmean_score_time                                          0.008416\nmean_test_recall                                         0.877206\nstd_test_recall                                          0.069196\nmean_test_precision                                           1.0\nstd_test_precision                                            0.0\nrank_test_recall                                                3\nrank_test_precision                                             1\nparams                 {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\nName: 2, dtype: object\n\n\nGridSearchCV(estimator=SVC(),\n             param_grid=[{'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001],\n                          'kernel': ['rbf']},\n                         {'C': [1, 10, 100, 1000], 'kernel': ['linear']}],\n             refit=&lt;function refit_strategy at 0x131cf09a0&gt;,\n             scoring=['precision', 'recall'])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(estimator=SVC(),\n             param_grid=[{'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001],\n                          'kernel': ['rbf']},\n                         {'C': [1, 10, 100, 1000], 'kernel': ['linear']}],\n             refit=&lt;function refit_strategy at 0x131cf09a0&gt;,\n             scoring=['precision', 'recall'])estimator: SVCSVC()SVCSVC()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Capítulo 5</span>"
    ]
  },
  {
    "objectID": "5_redes_neuronales_y_aprendizaje_profundo.html",
    "href": "5_redes_neuronales_y_aprendizaje_profundo.html",
    "title": "6  Redes neurales",
    "section": "",
    "text": "6.1 Perceptrón y redes neuronales\nEl perceptrón en el modelo más secillo de las neuronas. Se le llama también neurona artificial.\nLa idea es que cada uno de los features se multiplica por un peso \\(w_k\\), se le suma un bias \\(b\\) y al resultado de esta operación se le aplica la función de activación, que finalmente produce la salida \\(y\\). Esta función de activación preferiblemente debe ser diferenciable y tres de las funciones más comunes son la función logística, la función tangente hiperbólica y la función lineal rectificada unitaria (ReLU):\n\\[\\begin{equation*}\n\\text{logística: } f(x) = \\frac{1}{1+e^{-x}}\n\\end{equation*}\\]\n\\[\\begin{equation*}\n\\text{tangente hiperbólica: } f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n\\end{equation*}\\]\n\\[\\begin{equation*}\n\\text{ReLU: } f(x) = \\begin{cases}\n    0, & \\text{si } x &lt; 0 \\\\\n    x, & \\text{en otro caso}\n\\end{cases}\n\\end{equation*}\\]\nEn sí, el perceptrón es clasificador muy similar a la regresión logística. Sin embargo, es muy poco común que se utilice un solo perceptrón, sino que se utiliza varias capas (layers) de múltiples perceptrones, de manera que se pueda clasificar casos más complejos, a esto se le llama Perceptrón de capas múltiples (Multiple Layer Perceptron, MLP).\nUna capa está compuesta por varios perceptrones que están conectados con las entradas o con las salidas de la capa anterior, tal y como se muestra en la figura siguiente:\ncada uno de los bloques \\(P_{ij}\\) es un perceptrón. En la figura, solo se tiene una salida, pero bien podría tenerse más, por lo que la capa de salida podría tener más de un perceptrón. De igual manera, se puede tener más de una capa escondida.\nLo que se debe hacer ahora es que, a partir de los datos que se tienen, encontrar los valores de los pesos \\(w_k\\) y \\(b_k\\), es decir, entrenar a la red.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Redes neurales y Aprendizaje profundo</span>"
    ]
  },
  {
    "objectID": "5_redes_neuronales_y_aprendizaje_profundo.html#entrenamiento-de-la-red-neuronal",
    "href": "5_redes_neuronales_y_aprendizaje_profundo.html#entrenamiento-de-la-red-neuronal",
    "title": "6  Redes neurales",
    "section": "6.2 Entrenamiento de la red neuronal",
    "text": "6.2 Entrenamiento de la red neuronal\nPara entrenar la red, se debe buscar los valores. Para ello se minimiza una función de costo, como por ejemplo el error cuadrático medio.\nPara lograr la optimización se suele utilizar el descenso del gradiente, en un algoritmo llamado Backpropagation. Con el Backpropagation se calcula el gradiente de la función de costo con respecto a los pesos de la red, de una manera eficiente. Se calculan el gradiente una capa a la vez , iterando hacia atrás desde la última capa.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Redes neurales y Aprendizaje profundo</span>"
    ]
  },
  {
    "objectID": "5_redes_neuronales_y_aprendizaje_profundo.html#comandos-en-python",
    "href": "5_redes_neuronales_y_aprendizaje_profundo.html#comandos-en-python",
    "title": "6  Redes neurales",
    "section": "6.3 Comandos en python",
    "text": "6.3 Comandos en python\nCon la libraría scikit se puede crear y entrenar fácilmente una red neuronal.\n\nfrom sklearn.neural_network import MLPClassifier\n\n# Acá se cargan los datos\nX = [[0., 0.], [1., 1.]] # acá se pondría una lista de listas con los featrures\nY = [0, 1] # acá los labels\n\n# Acá se crea la red\nclf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n\n# Acá se entrena la red\nclf.fit(X, Y)\n\n# Y acá se utiliza la red para predecir el valor de la salida para una nueva entrada\nclf.predict([[-1., -2.]])\n\narray([0])\n\n\nen la variable hidden_layer_sizes se pone el número de perceptrones para cada una de las capas escondidas. alpha es la fuerza del término de regularización L2. El solver es el algoritmo de optimización:\n\nlbfgs es un optimizador de la familia de métodos cuasi-Newton.\nsgd se refiere al descenso estocástico del gradiente.\nadam se refiere al optimizador estocástico del gradiente propuesto por Kingma, Diederik, and Jimmy Ba.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Redes neurales y Aprendizaje profundo</span>"
    ]
  },
  {
    "objectID": "6_problemas_y_soluciones.html",
    "href": "6_problemas_y_soluciones.html",
    "title": "7  Problemas y soluciones",
    "section": "",
    "text": "7.1 Regresión de Kernel\nLa regresión de Kernel es utilizada para los casos, que la entrada es un vector de características D-dimensional, con D &gt; 3. La regresión del kernel es un método no paramétrico. Eso significa que no hay parámetros que aprender. El modelo se basa en los datos mismos (como en kNN). En su forma más simple, en la regresión del kernel buscamos un modelo como este:\n\\[\nf(x) = \\dfrac{1}{N}\\sum_{i=1}^{N}w_{i}y_{i}\n\\]\n, donde \\[\nw_{i} = \\dfrac{Nk(\\dfrac{x_{i}-x}{b}))}{\\sum_{i=1}^{N}k(\\dfrac{x_{k}-x}{b})}\n\\]\nLa función k(·) es un núcleo(kernel). Puede tener diferentes formas, la más utilizada es el núcleo gaussiano:\n\\[\nk(z)= \\dfrac{1}{\\sqrt{2\\pi}}exp(\\dfrac{-z^2}{2}).\n\\]\nEl valor b es un hiperparámetro que ajustamos usando el conjunto de validación.\nFuente: https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_compare_gpr_krr.html#sphx-glr-auto-examples-gaussian-process-plot-compare-gpr-krr-py",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Problemas y soluciones</span>"
    ]
  },
  {
    "objectID": "6_problemas_y_soluciones.html#regresión-de-kernel",
    "href": "6_problemas_y_soluciones.html#regresión-de-kernel",
    "title": "7  Problemas y soluciones",
    "section": "",
    "text": "Ejemplo 7.1  \n\nimport numpy as np\n\nrng = np.random.RandomState(0)\ndata = np.linspace(0, 30, num=1_000).reshape(-1, 1)\ntarget = np.sin(data).ravel()\n\n\ntraining_sample_indices = rng.choice(np.arange(0, 400), size=40, replace=False)\ntraining_data = data[training_sample_indices]\ntraining_noisy_target = target[training_sample_indices] + 0.5 * rng.randn(\n    len(training_sample_indices)\n)\n\n\nimport matplotlib.pyplot as plt\n\nplt.plot(data, target, label=\"True signal\", linewidth=2)\nplt.scatter(\n    training_data,\n    training_noisy_target,\n    color=\"black\",\n    label=\"Noisy measurements\",\n)\nplt.legend()\nplt.xlabel(\"data\")\nplt.ylabel(\"target\")\n_ = plt.title(\n    \"Illustration of the true generative process and \\n\"\n    \"noisy measurements available during training\"\n)\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import Ridge\n\nridge = Ridge().fit(training_data, training_noisy_target)\n\nplt.plot(data, target, label=\"True signal\", linewidth=2)\nplt.scatter(\n    training_data,\n    training_noisy_target,\n    color=\"black\",\n    label=\"Noisy measurements\",\n)\nplt.plot(data, ridge.predict(data), label=\"Lineal regression\")\nplt.legend()\nplt.xlabel(\"data\")\nplt.ylabel(\"target\")\n_ = plt.title(\"Limitation of a linear model such as ridge\")\n\n\n\n\n\n\n\n\n\nimport time\n\nfrom sklearn.gaussian_process.kernels import ExpSineSquared\nfrom sklearn.kernel_ridge import KernelRidge\n\nkernel_ridge = KernelRidge(kernel=ExpSineSquared())\n\nstart_time = time.time()\nkernel_ridge.fit(training_data, training_noisy_target)\nprint(\n    f\"Fitting KernelRidge with default kernel: {time.time() - start_time:.3f} seconds\"\n)\n\nplt.plot(data, target, label=\"True signal\", linewidth=2, linestyle=\"dashed\")\nplt.scatter(\n    training_data,\n    training_noisy_target,\n    color=\"black\",\n    label=\"Noisy measurements\",\n)\nplt.plot(\n    data,\n    kernel_ridge.predict(data),\n    label=\"Kernel ridge\",\n    linewidth=2,\n    linestyle=\"dashdot\",\n)\nplt.legend(loc=\"lower right\")\nplt.xlabel(\"data\")\nplt.ylabel(\"target\")\n_ = plt.title(\n    \"Kernel ridge regression with an exponential sine squared\\n \"\n    \"kernel using default hyperparameters\"\n)\n\nFitting KernelRidge with default kernel: 0.002 seconds\n\n\n\n\n\n\n\n\n\nEste modelo ajustado no es exacto. De hecho, no configuramos los parámetros del kernel y en su lugar utilizamos los predeterminados. Podemos inspeccionarlos.\n\nkernel_ridge.kernel\n\nExpSineSquared(length_scale=1, periodicity=1)\n\n\n\nfrom scipy.stats import loguniform\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nparam_distributions = {\n    \"alpha\": loguniform(1e0, 1e3),\n    \"kernel__length_scale\": loguniform(1e-2, 1e2),\n    \"kernel__periodicity\": loguniform(1e0, 1e1),\n}\nkernel_ridge_tuned = RandomizedSearchCV(\n    kernel_ridge,\n    param_distributions=param_distributions,\n    n_iter=500,\n    random_state=0,\n)\nstart_time = time.time()\nkernel_ridge_tuned.fit(training_data, training_noisy_target)\nprint(f\"Time for KernelRidge fitting: {time.time() - start_time:.3f} seconds\")\n\nTime for KernelRidge fitting: 5.883 seconds\n\n\nAjustar el modelo ahora es más costoso desde el punto de vista computacional ya que tenemos que probar varias combinaciones de hiperparámetros. Podemos echar un vistazo a los hiperparámetros encontrados para hacer algunas intuiciones.\n\nkernel_ridge_tuned.best_params_\n\n{'alpha': 1.991584977345022,\n 'kernel__length_scale': 0.7986499491396734,\n 'kernel__periodicity': 6.6072758064261095}\n\n\n\nstart_time = time.time()\npredictions_kr = kernel_ridge_tuned.predict(data)\nprint(f\"Time for KernelRidge predict: {time.time() - start_time:.3f} seconds\")\n\nplt.plot(data, target, label=\"True signal\", linewidth=2, linestyle=\"dashed\")\nplt.scatter(\n    training_data,\n    training_noisy_target,\n    color=\"black\",\n    label=\"Noisy measurements\",\n)\nplt.plot(\n    data,\n    predictions_kr,\n    label=\"Kernel ridge\",\n    linewidth=2,\n    linestyle=\"dashdot\",\n)\nplt.legend(loc=\"lower right\")\nplt.xlabel(\"data\")\nplt.ylabel(\"target\")\n_ = plt.title(\n    \"Kernel ridge regression with an exponential sine squared\\n \"\n    \"kernel using tuned hyperparameters\"\n)\n\nTime for KernelRidge predict: 0.006 seconds",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Problemas y soluciones</span>"
    ]
  },
  {
    "objectID": "6_problemas_y_soluciones.html#clasificación-multiclase",
    "href": "6_problemas_y_soluciones.html#clasificación-multiclase",
    "title": "7  Problemas y soluciones",
    "section": "7.2 Clasificación multiclase",
    "text": "7.2 Clasificación multiclase\nLa clasificación multiclase se refiere a aquellos casos en los que los datos contienen etiquetas que pertenecen a una de las \\(C\\) clases:\n\\[\ny \\in \\{1,...,C\\}\n\\]\nPor ejemplo, se puede clasificar utilizando features extraídos de un set de imágenes de frutas. En este ejemplo las etiquetas y serían:\n\ny = [\"manzana\", \"pera\", \"naranja\"]\n\nCada imagen es una muestra y puede ser clasificada como una de las tres posibles clases. La clasificación multiclase asume que cada muestra está asociada a una y solo una de las etiquetas.\nEn el ejemplo, una fotografía no podría ser una pera y una naranja al mismo tiempo. Si esto no se cumple estaríamos ante un ejemplo de clasificación multietiqueta, que se verá más adelante.\nExisten algunos algoritmos de clasificación que se pueden extender para ser algoritmos de clasificación multiclase:\n\nID3 y otros algoritmos de árboles de decisión\nRegresión logística reemplazando la función sigmoidal con la función softmax\nkNN\n\nHay otros algoritmos que no se pueden extender a clasificación multiclase de forma simple, o en algunos casos, son mucho más eficientes en el caso de clasificación binaria. Ante esta situación, una estrategia común es llamada uno versus el resto (OVR).\n\n7.2.1 Uno versus el resto (OVR)\nLa idea detrás del enfoque de OVR es separar el problema de clasificación multiclase en múltiples casos de separación binaria.\nEn la siguiente figura podemos observar una ilustración con dos tipos de problemas de clasificación: binaria y multiclase.\n\n\n\nIlustración de problemas de clasificación\n\n\nPara la imagen de la derecha, un ejemplo de clasificación multiclase, podemos utilizar la estrategia de OVR, tal y como se muestra en la siguiente figura.\n\n\n\nIlustración de la clasificación multiclase\n\n\n\n\n7.2.2 Implementación en python\nImports:\n\nimport pandas as pd\nimport seaborn as sns\n\nimport numpy as np\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.datasets import make_classification\n\nimport time\n\nCreando datos aleatorios para clasificación:\n\n# Generando un array aleatorio de 1000 muestras, 10 features y una etiqueta de y=[0,1,2]\nX, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, n_classes=3, random_state=1)\n\n# Separando el array en conjuntos de prueba (25 %) y entrenamiento (75 %)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n\n# Colocando en dataframes para facilidad de presentación y graficación\ndf_train = pd.DataFrame({\"y\":y_train, \"x0\":X_train[:,0], \"x1\":X_train[:,1], \"x2\":X_train[:,2], \"x3\":X_train[:,3], \"x4\":X_train[:,4], \"x5\":X_train[:,5], \"x6\":X_train[:,6], \"x7\":X_train[:,7], \"x8\":X_train[:,8], \"x9\":X_train[:,9]})\n\ndf_test = pd.DataFrame({\"y\":y_test, \"x0\":X_test[:,0], \"x1\":X_test[:,1], \"x2\":X_test[:,2], \"x3\":X_test[:,3], \"x4\":X_test[:,4], \"x5\":X_test[:,5], \"x6\":X_test[:,6], \"x7\":X_test[:,7], \"x8\":X_test[:,8], \"x9\":X_test[:,9]})\n\ndf_train\n\n\n\n\n\n\n\n\ny\nx0\nx1\nx2\nx3\nx4\nx5\nx6\nx7\nx8\nx9\n\n\n\n\n0\n2\n-0.730468\n0.397713\n2.074811\n3.268605\n2.406136\n-1.596750\n-1.097254\n0.839374\n0.296566\n-0.135522\n\n\n1\n2\n3.365775\n-1.418898\n1.872383\n-0.693364\n-3.133404\n-2.877166\n0.404063\n-1.955554\n-1.324028\n1.988373\n\n\n2\n2\n-3.921299\n0.641428\n-2.026409\n-2.679728\n2.478175\n4.280245\n0.959757\n-1.031408\n2.619929\n-1.552321\n\n\n3\n2\n-0.476039\n-0.361355\n0.966933\n2.514187\n3.982157\n-1.931150\n-0.354184\n2.025803\n-0.295005\n-2.314401\n\n\n4\n0\n-0.119220\n-0.421085\n0.706323\n0.561597\n-0.483120\n0.058308\n-0.852335\n-0.952946\n-0.161098\n0.587163\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n745\n2\n-0.707565\n-4.271104\n1.374571\n-0.328230\n6.264860\n-2.072022\n0.239431\n-1.271852\n-0.637552\n-5.091475\n\n\n746\n1\n-0.514813\n-0.473396\n-2.421689\n-1.699244\n-0.952055\n2.352776\n-0.218440\n0.078120\n-0.600584\n-0.977306\n\n\n747\n0\n2.273762\n1.123171\n0.927239\n2.014521\n-1.140324\n-2.785479\n0.158036\n2.155000\n-0.954108\n1.196839\n\n\n748\n0\n5.365061\n-7.979364\n3.641793\n0.967290\n3.271363\n-8.028106\n-0.642597\n-1.656500\n-5.257335\n-4.871621\n\n\n749\n1\n0.202570\n1.402667\n1.272342\n3.307607\n-1.209962\n-0.738314\n-1.770443\n1.023892\n-0.361837\n1.986999\n\n\n\n\n750 rows × 11 columns\n\n\n\n\nax = sns.scatterplot(data=df_train, x=\"x0\", y=\"x1\", hue=\"y\", palette=\"tab10\")\n\n\n\n\n\n\n\n\nDefiniendo el modelo OVR\n\n# Se define el modelo a usar dentro del OVR, en este caso SVC, puede ser logistico u otro\nmodelo = SVC()\n# Entrenando el modelo con los datos de entrenamiento\nclasificador = OneVsRestClassifier(modelo).fit(X_train, y_train)\n\nProbando el modelo OVR clasificador\n\n# Probando el modelo con los datos de prueba\nprediccion = clasificador.predict(X_test)\n\n# Colocando los datos predichos en un dataframe\ndf_pred = pd.DataFrame({\"y\":prediccion, \"x0\":X_test[:,0], \"x1\":X_test[:,1], \"x2\":X_test[:,2], \"x3\":X_test[:,3], \"x4\":X_test[:,4], \"x5\":X_test[:,5], \"x6\":X_test[:,6], \"x7\":X_test[:,7], \"x8\":X_test[:,8], \"x9\":X_test[:,9]})\n\n¿Cuál es el resultado del modelo clasificador?\nNos da un vector con las etiquetas predichas:\n\nprint(prediccion)\n\n[2 0 2 2 0 0 0 1 0 2 2 2 2 0 1 2 1 0 0 0 1 2 2 2 2 1 0 1 0 0 1 0 1 2 0 1 1\n 0 1 2 0 1 0 2 2 0 0 0 2 2 1 1 1 0 2 2 1 0 2 0 0 0 0 2 2 1 1 2 2 1 1 1 2 0\n 0 0 1 0 0 2 1 2 0 1 1 0 1 0 2 2 0 2 1 2 2 0 2 2 1 0 1 2 1 2 2 0 2 1 1 0 2\n 0 0 0 2 2 2 2 0 2 0 0 1 2 2 2 0 1 0 0 0 2 0 0 0 0 1 0 1 2 0 1 0 1 2 2 1 2\n 1 0 2 2 1 1 2 1 1 0 1 2 2 0 2 0 0 1 0 0 1 0 1 0 1 0 2 1 2 0 0 0 0 1 0 1 0\n 2 0 2 2 0 0 1 0 2 0 0 1 2 0 2 0 1 2 2 1 2 1 2 2 2 2 0 1 1 2 2 1 0 2 0 2 2\n 0 0 1 0 0 0 1 0 1 1 1 0 2 1 0 0 0 0 1 2 1 1 1 0 0 1 1 2]\n\n\nY con el método score(X, y) podemos obtener la mean accuracy, que es una métrica bastante exigente pues requiere que para cada muestra la etiqueta sea asignada correctamente:\n\nprint(\"mean accuracy = \",clasificador.score(X_test, y_test))\n\nmean accuracy =  0.908\n\n\nLuego, podemos comparar la gráfica del conjunto de prueba df_test, utilizando por ejemplo los features \\(x_0\\) y \\(x_1\\):\n\nax2 = sns.scatterplot(data=df_test, x=\"x0\", y=\"x1\", hue=\"y\", palette=\"tab10\")\n\n\n\n\n\n\n\n\nCon la gráfica utilizando los mismos features \\(x_0\\) y \\(x_1\\) del conjunto de prueba pero esta vez con las etiquetas predichas por el modelo. Se marca una x roja sobre los puntos que fueron clasificados incorrectamente:\n\nax2 = sns.scatterplot(data=df_pred, x=\"x0\", y=\"x1\", hue=\"y\", palette=\"tab10\")\n# Compara los y de prueba vs. los y predichos para marcar los que no se clasificaron correctamente\nfor i in range(len(prediccion)):\n    if prediccion[i] != y_test[i]:\n        ax2.plot(df_pred[\"x0\"].iloc[i], df_pred[\"x1\"].iloc[i], \"rx\")",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Problemas y soluciones</span>"
    ]
  },
  {
    "objectID": "6_problemas_y_soluciones.html#clasificación-con-una-clase",
    "href": "6_problemas_y_soluciones.html#clasificación-con-una-clase",
    "title": "7  Problemas y soluciones",
    "section": "7.3 Clasificación con una clase",
    "text": "7.3 Clasificación con una clase\n\n# Generate and plot a synthetic imbalanced classification dataset\nfrom collections import Counter\nfrom sklearn.datasets import make_classification\nfrom matplotlib import pyplot\nfrom numpy import where\n\n# define dataset\nX, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n n_clusters_per_class=1, weights=[0.999], flip_y=0, random_state=4)\n# summarize class distribution\ncounter = Counter(y)\nprint(counter)\n# scatter plot of examples by class label\nfor label, _ in counter.items():\n row_ix = where(y == label)[0]\n pyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\npyplot.legend()\npyplot.show()\n\nCounter({0: 9990, 1: 10})\n\n\n\n\n\n\n\n\n\nSe ajusta el modelo correspondiente\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom sklearn.svm import OneClassSVM\n# split into train/test sets\ntrainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=2, stratify=y)\n# define outlier detection model\nmodel = OneClassSVM(gamma='scale', nu=0.01)\n# fit on majority class\ntrainX_clean = trainX[trainy==0]\nmodel.fit(trainX_clean)\n# detect outliers in the test set\nyhat = model.predict(testX)\n# mark inliers 1, outliers -1\ntesty_clean = testy.copy()\ntesty_clean[testy == 1] = -1\ntesty_clean[testy == 0] = 1\n# calculate score\nscore = f1_score(testy_clean, yhat, pos_label=-1)\nprint('F1 Score: %.3f' % score)\n\nF1 Score: 0.123\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n# define the meshgrid\nx_min, x_max = trainX[:, 0].min() - 5, trainX[:, 0].max() + 5\ny_min, y_max = trainX[:, 1].min() - 5, trainX[:, 1].max() + 5\n\nx_ = np.linspace(x_min, x_max, 500)\ny_ = np.linspace(y_min, y_max, 500)\n\nxx, yy = np.meshgrid(x_, y_)\n\n# evaluate the decision function on the meshgrid\nz = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\nz = z.reshape(xx.shape)\n\n# plot the decision function and the reduced data\nplt.contourf(xx, yy, z, cmap=plt.cm.PuBu)\na = plt.contour(xx, yy, z, levels=[0], linewidths=2, colors='darkred')\nb = plt.scatter(trainX[trainy == 0, 0], trainX[trainy == 0, 1], c='white', edgecolors='k')\nc = plt.scatter(trainX[trainy == 1, 0], trainX[trainy == 1, 1], c='gold', edgecolors='k')\nplt.legend([a.collections[0], b, c], ['learned frontier', 'regular observations', 'abnormal observations'], bbox_to_anchor=(1.05, 1))\nplt.axis('tight')\nplt.show()\n\n/var/folders/4d/qj4qr8zx1n36td0hlt0p7x_h0000gn/T/ipykernel_24081/4105701295.py:21: MatplotlibDeprecationWarning:\n\nThe collections attribute was deprecated in Matplotlib 3.8 and will be removed two minor releases later.\n\n\n\n\n\n\n\n\n\n\n\nimport time\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn import svm\nfrom sklearn.covariance import EllipticEnvelope\nfrom sklearn.datasets import make_blobs, make_moons\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.kernel_approximation import Nystroem\nfrom sklearn.linear_model import SGDOneClassSVM\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.pipeline import make_pipeline\n\nmatplotlib.rcParams[\"contour.negative_linestyle\"] = \"solid\"\n\n# Example settings\nn_samples = 300\noutliers_fraction = 0.15\nn_outliers = int(outliers_fraction * n_samples)\nn_inliers = n_samples - n_outliers\n\n# define outlier/anomaly detection methods to be compared.\n# the SGDOneClassSVM must be used in a pipeline with a kernel approximation\n# to give similar results to the OneClassSVM\nanomaly_algorithms = [\n    (\n        \"Robust covariance\",\n        EllipticEnvelope(contamination=outliers_fraction, random_state=42),\n    ),\n    (\"One-Class SVM\", svm.OneClassSVM(nu=outliers_fraction, kernel=\"rbf\", gamma=0.1)),\n    (\n        \"One-Class SVM (SGD)\",\n        make_pipeline(\n            Nystroem(gamma=0.1, random_state=42, n_components=150),\n            SGDOneClassSVM(\n                nu=outliers_fraction,\n                shuffle=True,\n                fit_intercept=True,\n                random_state=42,\n                tol=1e-6,\n            ),\n        ),\n    ),\n    (\n        \"Isolation Forest\",\n        IsolationForest(contamination=outliers_fraction, random_state=42),\n    ),\n    (\n        \"Local Outlier Factor\",\n        LocalOutlierFactor(n_neighbors=35, contamination=outliers_fraction),\n    ),\n]\n\n# Define datasets\nblobs_params = dict(random_state=0, n_samples=n_inliers, n_features=2)\ndatasets = [\n    make_blobs(centers=[[0, 0], [0, 0]], cluster_std=0.5, **blobs_params)[0],\n    make_blobs(centers=[[2, 2], [-2, -2]], cluster_std=[0.5, 0.5], **blobs_params)[0],\n    make_blobs(centers=[[2, 2], [-2, -2]], cluster_std=[1.5, 0.3], **blobs_params)[0],\n    4.0\n    * (\n        make_moons(n_samples=n_samples, noise=0.05, random_state=0)[0]\n        - np.array([0.5, 0.25])\n    ),\n    14.0 * (np.random.RandomState(42).rand(n_samples, 2) - 0.5),\n]\n\n# Compare given classifiers under given settings\nxx, yy = np.meshgrid(np.linspace(-7, 7, 150), np.linspace(-7, 7, 150))\n\nplt.figure(figsize=(len(anomaly_algorithms) * 2 + 4, 12.5))\nplt.subplots_adjust(\n    left=0.02, right=0.98, bottom=0.001, top=0.96, wspace=0.05, hspace=0.01\n)\n\nplot_num = 1\nrng = np.random.RandomState(42)\n\nfor i_dataset, X in enumerate(datasets):\n    # Add outliers\n    X = np.concatenate([X, rng.uniform(low=-6, high=6, size=(n_outliers, 2))], axis=0)\n\n    for name, algorithm in anomaly_algorithms:\n        t0 = time.time()\n        algorithm.fit(X)\n        t1 = time.time()\n        plt.subplot(len(datasets), len(anomaly_algorithms), plot_num)\n        if i_dataset == 0:\n            plt.title(name, size=18)\n\n        # fit the data and tag outliers\n        if name == \"Local Outlier Factor\":\n            y_pred = algorithm.fit_predict(X)\n        else:\n            y_pred = algorithm.fit(X).predict(X)\n\n        # plot the levels lines and the points\n        if name != \"Local Outlier Factor\":  # LOF does not implement predict\n            Z = algorithm.predict(np.c_[xx.ravel(), yy.ravel()])\n            Z = Z.reshape(xx.shape)\n            plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors=\"black\")\n\n        colors = np.array([\"#377eb8\", \"#ff7f00\"])\n        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[(y_pred + 1) // 2])\n\n        plt.xlim(-7, 7)\n        plt.ylim(-7, 7)\n        plt.xticks(())\n        plt.yticks(())\n        plt.text(\n            0.99,\n            0.01,\n            (\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\n            transform=plt.gca().transAxes,\n            size=15,\n            horizontalalignment=\"right\",\n        )\n        plot_num += 1\n\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Problemas y soluciones</span>"
    ]
  },
  {
    "objectID": "6_problemas_y_soluciones.html#clasificación-multi-etiqueta",
    "href": "6_problemas_y_soluciones.html#clasificación-multi-etiqueta",
    "title": "7  Problemas y soluciones",
    "section": "7.4 Clasificación multi-etiqueta",
    "text": "7.4 Clasificación multi-etiqueta\nEn la clasificación multietiqueta, cada ejemplo de entrenamiento no tiene sólo una etiqueta, sino varias. de ellas. Por ejemplo, si queremos describir una imagen, podríamos asignarle varias etiquetas: “gente”, “concierto”, “naturaleza”, o las tres a la vez.\nSi el número de valores posibles para las etiquetas es elevado, pero todos son de la misma naturaleza, se puede optar por transformar cada ejemplo etiquetado en varios ejemplos etiquetados, uno por etiqueta. Estos nuevos ejemplos tienen todos el mismo vector de características y una sola etiqueta. Esto se convierte en un problema de clasificación multiclase.Se puede resolver utilizando la estrategia de uno contra el resto. La única diferencia con el problema multiclase habitual es que se genera un nuevo hiperparámetro: Threshold (umbral).\nSi la puntuación de predicción para alguna etiqueta está por encima del umbral, esta etiqueta se predice para el vector de características de entrada. En este escenario, se pueden predecir múltiples etiquetas para un solo vector de características.El valor del umbral se elige utilizando el conjunto de validación. De forma análoga, los algoritmos que pueden convertirse de forma natural en multiclase (árboles de decisión, regresión logística y redes neuronales, entre otros) pueden aplicarse a problemas de clasificación multietiqueta, ya que, estos devuelven la puntuación de cada clase, entonces se puede definir un umbral y asignar varias etiquetas a un vector de características si el umbral está por encima de un valor elegido, usando de forma experimental el conjunto de validación.\nLos algoritmos de redes neuronales pueden entrenar de forma natural modelos de clasificación multietiqueta utilizando la función de costo de entropía cruzada binaria. La capa de salida de la red neuronal, en este caso,tiene una unidad por etiqueta. Cada unidad de la capa de salida tiene la función de activación sigmoidea.\nEn los casos en los que el número de posibles valores que puede tomar cada etiqueta es pequeño, se puede converti en un problema multiclase utilizando un enfoque diferente. Imaginemos el siguiente problema. Se quiere etiquetar imágenes y las etiquetas pueden ser de dos tipos. El primer tipo de etiqueta puede tener dos valores posibles: {foto, pintura}; la etiqueta del segundo tipo puede tener tres valores posibles valores {retrato, paisaje, otro}. Se puede crear una nueva clase falsa para cada combinación de las dos clases originales, así:\n\n\n\n\n\nTabla\n\n\n\n\n\n\n\n\nClase falsa\n\n\nClase real 1\n\n\nClase real 2\n\n\n\n\n\n\n1\n\n\nFoto\n\n\nRetrato\n\n\n\n\n2\n\n\nFoto\n\n\nPaisaje\n\n\n\n\n3\n\n\nFoto\n\n\nOtro\n\n\n\n\n4\n\n\nPintura\n\n\nRetrato\n\n\n\n\n5\n\n\nPintura\n\n\nPaisaje\n\n\n\n\n6\n\n\nPintura\n\n\nOtro\n\n\n\n\n\n\nAhora se tienen los mismos ejemplos etiquetados, se sustituyen las etiquetas múltiples reales por una etiqueta falsa con valores de 1 a 6. Este enfoque funciona bien en la práctica cuando no hay demasiadas combinaciones posibles de clases. De lo contrario, es necesario utilizar muchos más datos de entrenamiento para compensar el aumento del número de clases.\nLa principal ventaja de este enfoque es que mantiene correlacionadas las etiquetas, al contrario que los métodos vistos anteriormente que predicen cada etiqueta independientemente de la otra. La correlación entre etiquetas puede ser una propiedad esencial en muchos problemas. Por ejemplo, si quiere predecir si un mensaje de correo electrónico es spam o no_spam al mismo tiempo que como predecir si es correo ordinario o prioritario.\n\n7.4.1 Ejemplo código\n\n7.4.1.1 Formato de destino\nUna representación válida de multi etiqueta es una matriz binaria y de forma densa o escasa . Cada columna representa una clase. Los 1’ en cada fila indican las clases positivas con las que se ha etiquetado una muestra. Un ejemplo de matriz densa para 3 muestras:(n_samples, n_classes)\n\nimport numpy as np\ny = np.array([[1, 0, 0, 1], [0, 0, 1, 1], [0, 0, 0, 0]])\nprint(y)\n\n[[1 0 0 1]\n [0 0 1 1]\n [0 0 0 0]]\n\n\nTambién se pueden crear matrices densas utilizando MultiLabelBinarizer\n\nimport numpy as np\nimport scipy.sparse as sparse\n\ny = np.array([[1, 0, 0, 1], [0, 0, 1, 1], [0, 0, 0, 0]])\ny_sparse = sparse.csr_matrix(y)\nprint(y_sparse)\n\n  (0, 0)    1\n  (0, 3)    1\n  (1, 2)    1\n  (1, 3)    1\n\n\n\nfrom sklearn.datasets import make_multilabel_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, hamming_loss\n\n# Generate synthetic multi-label dataset\nX, y = make_multilabel_classification(n_samples=100, n_features=10, n_classes=5, random_state=42)\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create a multi-label classifier\nclassifier = MultiOutputClassifier(KNeighborsClassifier())\n\n# Train the classifier\nclassifier.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = classifier.predict(X_test)\n\n# Calculate accuracy and Hamming loss\naccuracy = accuracy_score(y_test, y_pred)\nhamming_loss = hamming_loss(y_test, y_pred)\n\nprint(\"Hamming Loss:\", hamming_loss)\n\nHamming Loss: 0.21",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Problemas y soluciones</span>"
    ]
  },
  {
    "objectID": "7_practica_avanzada.html",
    "href": "7_practica_avanzada.html",
    "title": "8  Práctica avanzada",
    "section": "",
    "text": "8.1 Manejo de datos desbalanceados\nEl desbalance de clases en los conjuntos de datos es un desafío común en el aprendizaje automático, particularmente en aplicaciones como la detección de fraude, donde las clases de interés suelen estar subrepresentadas. Este desbalance puede sesgar el rendimiento de los modelos hacia la clase mayoritaria, resultando en una pobre clasificación de las instancias de la clase minoritaria.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Práctica avanzada</span>"
    ]
  },
  {
    "objectID": "7_practica_avanzada.html#manejo-de-datos-desbalanceados",
    "href": "7_practica_avanzada.html#manejo-de-datos-desbalanceados",
    "title": "8  Práctica avanzada",
    "section": "",
    "text": "8.1.1 Solución mediante Pesos Diferenciales en SVM\nEl SVM (Support Vector Machine) con margen blando permite manejar el desbalance mediante la asignación de un costo diferente a las clasificaciones erróneas de las clases. Matemáticamente, esto se refleja en la función de pérdida, donde el costo CC se ajusta por clase:\n\\[\\begin{equation*}\nL(y,f(x))=C_{clase}⋅max⁡(0,1−y⋅f(x))2\n\\end{equation*}\\]\ndonde \\(C_{clase}\\) es el peso asignado a la clase, \\(y\\) es la etiqueta verdadera, y \\(f(x)\\) es la decisión del modelo.\n\n8.1.1.1 Implementación\nVeamos cómo implementar un clasificador SVM que gestiona el desbalance de clases asignando pesos específicos en scikit-learn.\nPrimero, creemos un conjunto de datos desbalanceado y dividámoslo en conjuntos de entrenamiento y prueba.\n\nimport numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\n\n# Generamos un conjunto de datos desbalanceado\nX, y = make_classification(n_classes=2, class_sep=2,\n                           weights=[0.1, 0.90], n_informative=3, n_redundant=1, flip_y=0,\n                           n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)\n\n# Dividimos en conjunto de entrenamiento y prueba\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\nAhora, ajustaremos un modelo SVM considerando el desbalance mediante el uso de pesos de clase:\n\n# Definimos los pesos de las clases para tratar el desbalance\nweights = {0: 1000, 1: 1}  # Aumentamos el peso de la clase minoritaria\n\n# Creamos el clasificador SVM con los pesos de clase\nclf = SVC(kernel='linear', class_weight=weights)\n\n# Entrenamos el modelo\nclf.fit(X_train, y_train)\n\n# Evaluamos el modelo\naccuracy = clf.score(X_test, y_test)\nprint(f'Accuracy: {accuracy:.2f}')\n\nAccuracy: 0.98\n\n\n\n\n\n8.1.2 Solución #2: Submuestreo y Sobremuestreo\nSi el algoritmo de aprendizaje no permite la ponderación de clases, existen técnicas de muestreo como el sobremuestreo (oversampling), el submuestreo (undersampling), y la creación de ejemplos sintéticos mediante algoritmos como SMOTE o ADASYN para equilibrar las clases.\nSMOTE (Synthetic Minority Over-sampling Technique) es una técnica de sobremuestreo que crea ejemplos sintéticos de la clase minoritaria para equilibrar el conjunto de datos. Veamos cómo aplicarlo usando la biblioteca imbalanced-learn.\n\nimport matplotlib.pyplot as plt\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.metrics import classification_report\n\n# Aplicamos SMOTE al conjunto de entrenamiento\nsmote = SMOTE(random_state=42)\nX_res, y_res = smote.fit_resample(X_train, y_train)\n\n# Entrenamos un nuevo clasificador SVM con los datos sobremuestreados\nclf_smote = SVC(kernel='linear')\nclf_smote.fit(X_res, y_res)\n\n# Evaluamos el modelo\ny_pred = clf_smote.predict(X_test)\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.89      0.96      0.93        26\n           1       1.00      0.99      0.99       224\n\n    accuracy                           0.98       250\n   macro avg       0.94      0.97      0.96       250\nweighted avg       0.98      0.98      0.98       250\n\n\n\nComparamos el efecto antes y después de aplicar SMOTE:\n\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=plt.cm.coolwarm, edgecolors='k')\nplt.title('Antes de SMOTE')\n\nplt.subplot(1, 2, 2)\nplt.scatter(X_res[:, 0], X_res[:, 1], c=y_res, cmap=plt.cm.coolwarm, edgecolors='k')\nplt.title('Después de SMOTE')\n\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Práctica avanzada</span>"
    ]
  },
  {
    "objectID": "7_practica_avanzada.html#combinación-de-modelos",
    "href": "7_practica_avanzada.html#combinación-de-modelos",
    "title": "8  Práctica avanzada",
    "section": "8.2 Combinación de Modelos",
    "text": "8.2 Combinación de Modelos\nLa combinación de modelos en el aprendizaje automático es una técnica poderosa que busca mejorar el rendimiento predictivo al integrar las fortalezas de varios modelos. Existen diversas formas de combinar modelos, siendo las más comunes el promedio (averaging), el voto de mayoría (majority vote) y el apilamiento (stacking). Cada uno de estos métodos tiene aplicaciones específicas y beneficios únicos.\n\n8.2.1 Promedio (Averaging)\nEl método de promedio es aplicable tanto para la regresión como para la clasificación. Consiste en aplicar todos los modelos base al input \\(x\\) y luego promediar las predicciones. En clasificación, se promedian las probabilidades predichas para cada clase.\nConsideremos un conjunto de datos de regresión y combinemos las predicciones de varios modelos de regresión mediante el promedio.\n\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\n# Generamos un conjunto de datos de regresión\nX, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Entrenamos varios modelos\nmodel_rf = RandomForestRegressor(n_estimators=10, random_state=42).fit(X_train, y_train)\nmodel_gb = GradientBoostingRegressor(n_estimators=10, random_state=42).fit(X_train, y_train)\nmodel_lr = LinearRegression().fit(X_train, y_train)\n\n# Predecimos y promediamos las predicciones\npredictions = np.mean([model_rf.predict(X_test), model_gb.predict(X_test), model_lr.predict(X_test)], axis=0)\n\n# Evaluamos el rendimiento del modelo promedio\nfrom sklearn.metrics import mean_squared_error\nmse = mean_squared_error(y_test, predictions)\nmse_rf = mean_squared_error(y_test, model_rf.predict(X_test))\nmse_gb = mean_squared_error(y_test, model_gb.predict(X_test))\nmse_lr = mean_squared_error(y_test, model_lr.predict(X_test))\n\nprint(f'MSE del modelo RF: {mse_rf}')\nprint(f'MSE del modelo GB: {mse_gb}')\nprint(f'MSE del modelo LR: {mse_lr}')\nprint(f'MSE del modelo promediado: {mse}')\n\nMSE del modelo RF: 9445.970722326663\nMSE del modelo GB: 20390.298586678025\nMSE del modelo LR: 0.010704979443048707\nMSE del modelo promediado: 5842.696381684256\n\n\n\n\n8.2.2 Voto de mayoría (Majority Vote)\nEl voto de mayoría se utiliza para modelos de clasificación. Se aplica cada uno de los modelos base al input xx y se selecciona la clase que obtenga la mayoría de votos entre todas las predicciones.\nUtilizaremos varios clasificadores y combinaremos sus predicciones mediante el voto de mayoría.\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report\n\n# Generamos un conjunto de datos de clasificación\nX, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Entrenamos varios clasificadores\nmodel_rf = RandomForestClassifier(n_estimators=10, random_state=42)\nmodel_lr = LogisticRegression()\nmodel_svc = SVC(probability=True, random_state=42)\n\n# Combinamos mediante voto de mayoría\neclf = VotingClassifier(estimators=[('rf', model_rf), ('lr', model_lr), ('svc', model_svc)], voting='hard')\neclf.fit(X_train, y_train)\nmodel_rf.fit(X_train, y_train)\nmodel_lr.fit(X_train, y_train)\nmodel_svc.fit(X_train, y_train)\n\n\n# Evaluamos el rendimiento\naccuracy = eclf.score(X_test, y_test)\naccuracy_rf = model_rf.score(X_test, y_test)\naccuracy_lr = model_lr.score(X_test, y_test)    \naccuracy_svc = model_svc.score(X_test, y_test)\n\nprint(f'Accuracy del modelo RF: {accuracy_rf}')\nprint(f'Accuracy del modelo LR: {accuracy_lr}')\nprint(f'Accuracy del modelo SVC: {accuracy_svc}')\nprint(f'Accuracy del modelo combinado mediante voto de mayoría: {accuracy}')\n\ny_pred = eclf.predict(X_test)\nprint(classification_report(y_test, y_pred))\n\nprint(classification_report(y_test, model_rf.predict(X_test)))\n\nAccuracy del modelo RF: 0.8333333333333334\nAccuracy del modelo LR: 0.85\nAccuracy del modelo SVC: 0.8333333333333334\nAccuracy del modelo combinado mediante voto de mayoría: 0.8433333333333334\n              precision    recall  f1-score   support\n\n           0       0.81      0.88      0.84       145\n           1       0.88      0.81      0.84       155\n\n    accuracy                           0.84       300\n   macro avg       0.84      0.84      0.84       300\nweighted avg       0.85      0.84      0.84       300\n\n              precision    recall  f1-score   support\n\n           0       0.80      0.88      0.84       145\n           1       0.88      0.79      0.83       155\n\n    accuracy                           0.83       300\n   macro avg       0.84      0.83      0.83       300\nweighted avg       0.84      0.83      0.83       300\n\n\n\n\n\n8.2.3 Apilamiento (Stacking)\nEl apilamiento consiste en combinar varios modelos base y utilizar sus salidas como input para un meta-modelo, que hace la predicción final. Ejemplo en Python para Stacking\nImplementaremos stacking con varios modelos base y un meta-modelo de regresión logística.\n\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Definimos los modelos base y el meta-modelo\nbase_models = [('rf', RandomForestClassifier(n_estimators=10, random_state=42)),\n               ('svc', SVC(probability=True, random_state=42))]\nmeta_model = LogisticRegression()\n\n# Creamos el modelo de apilamiento\nstacking_model = StackingClassifier(estimators=base_models, final_estimator=meta_model)\n\n# Entrenamos y evaluamos el modelo de apilamiento\nstacking_model.fit(X_train, y_train)\n\naccuracy = stacking_model.score(X_test, y_test)\nprint(f'Accuracy del modelo de apilamiento: {accuracy}')\n\nprint(classification_report(y_test, stacking_model.predict(X_test)))\nprint(classification_report(y_test, model_rf.predict(X_test)))\nprint(classification_report(y_test, model_svc.predict(X_test)))\n\nAccuracy del modelo de apilamiento: 0.84\n              precision    recall  f1-score   support\n\n           0       0.82      0.86      0.84       145\n           1       0.86      0.82      0.84       155\n\n    accuracy                           0.84       300\n   macro avg       0.84      0.84      0.84       300\nweighted avg       0.84      0.84      0.84       300\n\n              precision    recall  f1-score   support\n\n           0       0.80      0.88      0.84       145\n           1       0.88      0.79      0.83       155\n\n    accuracy                           0.83       300\n   macro avg       0.84      0.83      0.83       300\nweighted avg       0.84      0.83      0.83       300\n\n              precision    recall  f1-score   support\n\n           0       0.81      0.86      0.83       145\n           1       0.86      0.81      0.83       155\n\n    accuracy                           0.83       300\n   macro avg       0.83      0.83      0.83       300\nweighted avg       0.83      0.83      0.83       300",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Práctica avanzada</span>"
    ]
  },
  {
    "objectID": "7_practica_avanzada.html#entrenamiento-de-redes-neuronales",
    "href": "7_practica_avanzada.html#entrenamiento-de-redes-neuronales",
    "title": "8  Práctica avanzada",
    "section": "8.3 Entrenamiento de Redes Neuronales",
    "text": "8.3 Entrenamiento de Redes Neuronales\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Crear un generador de datos con normalización\ndatagen = ImageDataGenerator(rescale=1./255)\n\n# Suponiendo que 'directorio_de_datos' es el camino a las imágenes\ntrain_generator = datagen.flow_from_directory(\n    directorio_de_datos,\n    target_size=(200, 200),  # Todas las imágenes se redimensionan a 200x200\n    batch_size=32,\n    class_mode='binary'  # o 'categorical' para clasificación multiclase\n)\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Crear un tokenizador para convertir palabras a índices\ntokenizer = Tokenizer(num_words=10000)  # Considera las 10,000 palabras más comunes\ntokenizer.fit_on_texts(textos)  # 'textos' es una lista de documentos de texto\n\n# Convertir textos en secuencias de índices\nsequences = tokenizer.texts_to_sequences(textos)\n\n# Acolchar secuencias para que tengan la misma longitud\ndata = pad_sequences(sequences, maxlen=100)  # Longitud fija de 100 para todas las secuencias\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\n\n# Crear un modelo simple como punto de partida\nmodel = Sequential([\n    Dense(64, activation='relu', input_shape=(100,)),  # Ejemplo para datos vectorizados de longitud 100\n    Dropout(0.5),  # Regularización mediante Dropout\n    Dense(1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# Entrenar el modelo\nmodel.fit(data, etiquetas, epochs=10, validation_split=0.2)  # 'etiquetas' es un array de etiquetas",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Práctica avanzada</span>"
    ]
  },
  {
    "objectID": "8_aprendizaje_no_supervisado.html",
    "href": "8_aprendizaje_no_supervisado.html",
    "title": "9  Aprendizaje no supervisado",
    "section": "",
    "text": "9.1 Estimación de densidad\nLa estimación de la densidad es un problema de modelar la función de densidad (pdf) de la distribución desconcida del dataset. Sus aplicaciones principales son en la ditección de novedades e intrusiones. Anteriormente se trabajó con la estimación de la pdf para el caso paramétrico con la distribución normal multivariada. Acá usaremos el método del kernel, ya que este es no paramétrico.\nAsí, sea \\(\\{x_i\\}_{i=1}^{N}\\) un dataset de una dimensión donde las muestras son construidas a partir de una pdf desconocida \\(f\\) con \\(x_i\\in\\mathbb{R}, \\ \\forall i=1, \\ldots, N\\). Estamos interesados en modelar la curva de la función \\(f\\). Con nuestro modelo de kernel, denotado por \\(\\hat{f}\\), defindo como: \\[\n\\hat{f}_h(x) = \\dfrac{1}{Nh}\\sum_{i=1}^{N} k\\left(\\dfrac{x-x_i}{h}\\right)\n\\tag{9.1}\\]\nDonde \\(h\\) es un hiperparámetro que controla la relación sesgo-varianza. Acá usaremos el kernel gaussiano: \\[\\begin{equation}\nk(z) = \\dfrac{1}{\\sqrt{2\\pi}}\\exp\\left(\\dfrac{-z^2}{2}\\right)\n\\end{equation}\\] Nosotros buscamos el valor de \\(h\\) que minimiza la diferencia entre la curva original \\(f\\) y la curva aproximada de nuestro modelo \\(f_{h}\\). Una medida razonable para esta diferencia es el error cuadrático medio integrado (MISE, por sus siglas en inglés), definido por: \\[\nMISE(b) = \\mathbb{E}\\left[\\int_{\\mathbb{R}}\\left(\\hat{f}_{h}(x)-f(x)\\right)^2dx\\right]\n\\tag{9.2}\\]\nEn la ecuación (Ecuación 9.2) la integral \\(\\int_{\\mathbb{R}}\\) remplaza a la sumatoria \\(\\displaystyle\\sum_{i=1}^{N}\\) que empleamos en el promedio, mientras que la esperanza \\(\\mathbb{E}\\) reemplaza el promedio \\(\\dfrac{1}{N}\\).\nNotese que cuando la función de pérdida es continua como la función de \\(\\left(\\hat{f}_{h}(x)-f(x)\\right)^2\\), se reemplaza la sumatoria por la integrasl. El operador de esperanza \\(\\mathbb{E}\\) siginifica que queremos que \\(h\\) sea el óptimo para todos las posibilidades del set de entrenamiento. Esto es importante debido a que \\(\\hat{f}_{h}\\) es definido en un conjunto finito de datos de alguna distribución de probabilidad; mientras que la pdf real \\(f\\) está definida en un dominio infinito \\(\\mathbb{R}\\).\nNote que, reescribiendo el lado derecho de la (Ecuación 9.2), obtenemos \\[\\begin{equation*}\n\\mathbb{E}\\left[\\int_{\\mathbb{R}}\\hat{f}_{h}^{2}(x)dx\\right]\n-2\\mathbb{E}\\left[\\int_{\\mathbb{R}}\\hat{f}_{h}(x)f(x)dx\\right]\n+ \\mathbb{E}\\left[\\int_{\\mathbb{R}}f^{2}(x)dx\\right]\n\\end{equation*}\\]\nNote que el tercer término es independiente de \\(h\\) y podría ser ignorado. Un estimador insesgado del primer término está dado por \\(\\int_{\\mathbb{R}}\\hat{f}_{b}^{2}(x)dx\\), mientras que el estimador insesgado para el segundo término está aproximado por \\(\\dfrac{-2}{N}\\displaystyle\\sum_{i=1}^{N}\\hat{f}_{h}^{(i)}(x_i)\\), donde \\(\\hat{f}_{h}^{(i)}(x_i)\\) es el kernel con los datos de entrenamiento menos el dato \\(x_i\\).\nEl término \\(\\displaystyle\\sum_{i=1}^{N}\\hat{f}_{h}^{(i)}(x_i)\\) es conocindo como el estimador de dejar una estimación por fuera (leave one out estimate); es una forma de validación cruzada donde cada fold contienen una muestra. Además, se puede ver como \\(\\int_{\\mathbb{R}}\\hat{f}_{h}(x)f(x)dx\\) es la esperanza de la función \\(\\hat{f}_{h}\\), esto por que \\(f\\) es una función de densidad. Se puede demostra que el estimador leave one out estimate es un estimador insesgado para \\(\\mathbb{E}\\left[\\int_{\\mathbb{R}}\\hat{f}_{h}(x)f(x)dx\\right]\\).\nAhora, para hallar el valor óptimo \\(h^*\\) para \\(h\\), queremos minimizar la función de costo definida por: \\[\n\\displaystyle\\int_{\\mathbb{R}}\\hat{f}*{h}^{2}(x)dx - \\dfrac{2}{N}\\displaystyle\\sum*{i=1}^{N}\\hat{f}_{h}^{(i)}(x_i)\n\\]\nSe puede hallar \\(h^*\\) utilizando grid search¨. Para \\(D\\) dimensiones, el término del error \\(x-x_i\\) de la (Ecuación 9.1) puede ser reemplazado por la norma euclidea \\(||\\mathbb{x}-\\mathbb{x}_{i}||\\).\n#Importar las librerías\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import norm\nfrom sklearn.neighbors import KernelDensity\n\n# ----------------------------------------------------------------------\n# Plot the progression of histograms to kernels\nnp.random.seed(1)\nN = 20\nX = np.concatenate(\n    (np.random.normal(0, 1, int(0.3 * N)), np.random.normal(5, 1, int(0.7 * N)))\n)[:, np.newaxis]\nX_plot = np.linspace(-5, 10, 1000)[:, np.newaxis]\nbins = np.linspace(-5, 10, 10)\n\nfig, ax = plt.subplots(2, 2, sharex=True, sharey=True)\nfig.subplots_adjust(hspace=0.05, wspace=0.05)\n\n# Histograma\nax[0, 0].hist(X[:, 0], bins=bins, fc=\"#AAAAFF\", density=True)\nax[0, 0].text(-3.5, 0.31, \"Histograma\")\n\n# Histograma con las particiones desplazadas\nax[0, 1].hist(X[:, 0], bins=bins + 0.75, fc=\"#AAAAFF\", density=True)\nax[0, 1].text(-3.5, 0.31, \"Histograma, bins desplazados\")\n\n# tophat KDE\nkde = KernelDensity(kernel=\"tophat\", bandwidth=0.75).fit(X)\nlog_dens = kde.score_samples(X_plot)\nax[1, 0].fill(X_plot[:, 0], np.exp(log_dens), fc=\"#AAAAFF\")\nax[1, 0].text(-3.5, 0.31, \"Tophat Kernel Density\")\n\n# Gaussian KDE\nkde = KernelDensity(kernel=\"gaussian\", bandwidth=0.75).fit(X)\nlog_dens = kde.score_samples(X_plot)\nax[1, 1].fill(X_plot[:, 0], np.exp(log_dens), fc=\"#AAAAFF\")\nax[1, 1].text(-3.5, 0.31, \"Gaussian Kernel Density\")\n\nfor axi in ax.ravel():\n    axi.plot(X[:, 0], np.full(X.shape[0], -0.01), \"+k\")\n    axi.set_xlim(-4, 9)\n    axi.set_ylim(-0.02, 0.34)\n\nfor axi in ax[:, 0]:\n    axi.set_ylabel(\"Normalized Density\")\n\nfor axi in ax[1, :]:\n    axi.set_xlabel(\"x\")\n\n# ----------------------------------------------------------------------\n# Plot all available kernels\nX_plot = np.linspace(-6, 6, 1000)[:, None]\nX_src = np.zeros((1, 1))\n\nfig, ax = plt.subplots(2, 3, sharex=True, sharey=True)\nfig.subplots_adjust(left=0.05, right=0.95, hspace=0.05, wspace=0.05)\n\n\ndef format_func(x, loc):\n    if x == 0:\n        return \"0\"\n    elif x == 1:\n        return \"h\"\n    elif x == -1:\n        return \"-h\"\n    else:\n        return \"%ih\" % x\n\n\nfor i, kernel in enumerate(\n    [\"gaussian\", \"tophat\", \"epanechnikov\", \"exponential\", \"linear\", \"cosine\"]\n):\n    axi = ax.ravel()[i]\n    log_dens = KernelDensity(kernel=kernel).fit(X_src).score_samples(X_plot)\n    axi.fill(X_plot[:, 0], np.exp(log_dens), \"-k\", fc=\"#AAAAFF\")\n    axi.text(-2.6, 0.95, kernel)\n\n    axi.xaxis.set_major_formatter(plt.FuncFormatter(format_func))\n    axi.xaxis.set_major_locator(plt.MultipleLocator(1))\n    axi.yaxis.set_major_locator(plt.NullLocator())\n\n    axi.set_ylim(0, 1.05)\n    axi.set_xlim(-2.9, 2.9)\n\nax[0, 1].set_title(\"Kernels Disponibles\")\n\n# ----------------------------------------------------------------------\n# Plot a 1D density example\nN = 100\nnp.random.seed(1)\nX = np.concatenate(\n    (np.random.normal(0, 1, int(0.3 * N)), np.random.normal(5, 1, int(0.7 * N)))\n)[:, np.newaxis]\n\nX_plot = np.linspace(-5, 10, 1000)[:, np.newaxis]\n\ntrue_dens = 0.3 * norm(0, 1).pdf(X_plot[:, 0]) + 0.7 * norm(5, 1).pdf(X_plot[:, 0])\n\nfig, ax = plt.subplots()\nax.fill(X_plot[:, 0], true_dens, fc=\"black\", alpha=0.2, label=\"input distribution\")\ncolors = [\"navy\", \"cornflowerblue\", \"darkorange\"]\nkernels = [\"gaussian\", \"tophat\", \"epanechnikov\"]\nlw = 2\n\nfor color, kernel in zip(colors, kernels):\n    kde = KernelDensity(kernel=kernel, bandwidth=0.5).fit(X)\n    log_dens = kde.score_samples(X_plot)\n    ax.plot(\n        X_plot[:, 0],\n        np.exp(log_dens),\n        color=color,\n        lw=lw,\n        linestyle=\"-\",\n        label=\"kernel = '{0}'\".format(kernel),\n    )\n\nax.text(6, 0.38, \"N={0} points\".format(N))\n\nax.legend(loc=\"upper left\")\nax.plot(X[:, 0], -0.005 - 0.01 * np.random.random(X.shape[0]), \"+k\")\n\nax.set_xlim(-4, 9)\nax.set_ylim(-0.02, 0.4)\nplt.show()\nLa agrupación es un problema de aprender a asignar una etiqueta a ejemplos aprovechando un no etiquetado conjunto de datos. Debido a que el conjunto de datos no está etiquetado en absoluto, decidir si el modelo aprendido es óptimo es mucho más complicado que en el aprendizaje supervisado. Existe una variedad de algoritmos de agrupamiento y, desafortunadamente, es difícil saber cuál es el mejor calidad para su conjunto de datos. Generalmente, el rendimiento de cada algoritmo depende de las propiedades desconocidas de la distribución de probabilidad de la que se extrajo el conjunto de datos.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Aprendizaje no supervisado</span>"
    ]
  },
  {
    "objectID": "8_aprendizaje_no_supervisado.html#k-medias",
    "href": "8_aprendizaje_no_supervisado.html#k-medias",
    "title": "9  Aprendizaje no supervisado",
    "section": "9.2 K-Medias",
    "text": "9.2 K-Medias\nEl algoritmo de agrupamiento de k-medias funciona de la siguiente manera:\nPrimero, el analista tiene que elegir k - el número de clases (o grupos). Luego colocamos aleatoriamente k vectores de características, llamados centroides, en el espacio de características.\nLuego calculamos la distancia desde cada ejemplo x a cada centroide usando alguna métrica, como la distancia euclidiana. Luego asignamos el centroide más cercano a cada ejemplo (como si etiquetáramos cada ejemplo con una identificación de centroide como etiqueta). Para cada centroide, calculamos el vector de características promedio de los ejemplos etiquetados con él. Estas características promedio los vectores se convierten en las nuevas ubicaciones de los centroides.\nEl valor de k, el número de clusters, es un hiperparámetro que los datos deben ajustar. Existen algunas técnicas para seleccionar k. Ninguno de ellos ha demostrado ser óptimo. La mayoría de requieren que el analista haga una “suposición fundamentada” observando algunas métricas o examinando visualmente las asignaciones de grupos. Más adelante en este capítulo, consideraremos una técnica lo que permite elegir un valor razonablemente bueno para k sin mirar los datos y hacer suposiciones.\n\n9.2.1 Ejemplo\nVisualización de datos\n\nimport matplotlib.pyplot as plt\n\nx = [4, 5, 10, 4, 3, 11, 14 , 6, 10, 12]\ny = [21, 19, 24, 17, 16, 25, 24, 22, 21, 21]\n\nplt.scatter(x, y)\nplt.show()\n\n\n\n\n\n\n\n\nMétodo del codo para seleccionar la cantidad de k:\n\nfrom sklearn.cluster import KMeans\n\ndata = list(zip(x, y))\ninertias = []\n\nfor i in range(1,11):\n    kmeans = KMeans(n_clusters=i, n_init=\"auto\")\n    kmeans.fit(data)\n    inertias.append(kmeans.inertia_)\n\nplt.plot(range(1,11), inertias, marker='o')\nplt.title('Elbow method')\nplt.xlabel('Number of clusters')\nplt.ylabel('Inertia')\nplt.show()\n\n\n\n\n\n\n\n\n\nkmeans = KMeans(n_clusters=2, n_init=\"auto\")\nkmeans.fit(data)\n\nplt.scatter(x, y, c=kmeans.labels_)\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Aprendizaje no supervisado</span>"
    ]
  },
  {
    "objectID": "8_aprendizaje_no_supervisado.html#reducción-de-dimensionalidad",
    "href": "8_aprendizaje_no_supervisado.html#reducción-de-dimensionalidad",
    "title": "9  Aprendizaje no supervisado",
    "section": "9.3 Reducción de dimensionalidad",
    "text": "9.3 Reducción de dimensionalidad\n\n9.3.1 Análisis de componentes principales (PCA)\n\n\n9.3.2 UMAP\nUMAP (Uniform Manifold Approximation and Projection) es un algoritmo de aprendizaje de manifolds para la reducción de dimensionalidad, superior a t-SNE por su eficiencia y versatilidad. Desarrollado por Leland McInnes, John Healy y James Melville en 2018, UMAP se fundamenta en el análisis topológico de datos, ofreciendo una metodología robusta para visualizar y analizar datos en alta dimensión.\nEl algoritmo construye representaciones topológicas de los datos mediante aproximaciones locales del manifold y uniendo estas representaciones en un conjunto simplicial difuso. Minimiza la entropía cruzada entre las representaciones topológicas de los espacios de alta y baja dimensión para lograr una proyección coherente.\nSe dará un resumen básico del método, sin embargo se recomienda leer el artículo original de UMAP para una comprensión más profunda.\n\n\n9.3.3 Análisis topológico de datos y complejos simpliciales\nGeometricamente, un \\(k\\)-simplex es un objeto \\(k\\)-dimensional que es simplemente la envolutra convexa de \\(k+1\\) puntos en un espacio \\(k\\)-dimensional. Un 0-simplex es un vértice, un 1-simplex es una arista, un 2-simplex es un triángulo, un 3-simplex es un tetraedro, etc.\n\nUn complejo simplicial \\(K\\) es una colección de simplexes que cumple con dos propiedades:\n\nCada cara de un simplex en \\(K\\) también está en \\(K\\).\nLa intersección de dos simplexes en \\(\\sigma_{1}, \\sigma_{2} \\in K\\) es una cara de ambos \\(\\sigma_{1}\\) y \\(\\sigma_{2}\\).\n\n\n\n\n9.3.4 Construcción intuitiva de UMAP\nUn conjunto de datos es solo una colección finita de puntos en un espacio. En general para entender las características topoloficas, necesitas crear una covertura abierta del espacio. Si los datos están en un espacio métrico, una forma de aproximar esas coverturas abiertas es con bolas abiertas alrededor de cada punto.\nPor ejemplo, suponga que se tiene un conjunto con esta forma:\n\nSi se toma cada punto y se dibuja una bola alrededor de él, se obtiene algo como esto:\n\nPodemos generar un complejo simplicial a través de un complejo Vietoris-Rips. Este complejo se construye tomando cada bola y creando una arista entre cada par de bolas que se superponen. Luego, se crean triángulos entre cada terna de bolas que se superponen, y así sucesivamente.\n\nEsto genera que ahora los datos estén representados a través de un grafo en baja dimensión.\n\n\n9.3.5 Adaptación del problema a datos reales\nProblema #1: Escogencia del radio\nLa técnica anterior tiene un problema, no sabemos de antemano el radio óptimo de las bolas. Entonces:\n\nRadio es muy pequeno -&gt; No se capturan las relaciones entre los puntos\nRadio es muy grande -&gt; Se pierde la estructura local de los datos\n\nSolución: Asumir que los datos son uniformes en la variedad\n\nEl problema es que este tipo de supuesto no es real para toda la variedad. El problema es que la noción de distancia varía de punto a punto. En algunos puntos es más largo otros más corto.\nSin embargo, podemos construir una aproximación de uniformidad local de los puntos usando la geometría Riemaniana. Esto es que la bola alrededor de un punto se extiende hasta los \\(k\\) vecinos más cercanos. Así que cada punto tendrá su propia función de distancia.\nDesde un punto topológico, \\(k\\) significa qué tanto queremos estimar la métrica Riemaniana localmente. Si \\(k\\) es pequeño se explicaría features muy locales. Si \\(k\\) es grande, el features sería más global.\n\n\n\n9.3.6 Un beneficio de la geometría Riemaniana\nSe puede tener un espacio métrico asociado con cada punto. Es decir, cada punto puede medir distancia de forma significativa de modo que se puede estimar el peso de las aristas del grafo con las distancias que se genera.\nAhora, pensemos que si en lugar de decir que la covertura fue una un “si” o “no”, fuera un concepto más difuso como un valor de 0 a 1. Entonces, a partir del cierto punto, el valor se vuelve mas cercano a 0 conforme nos alejamos de este.\n\nProblema #2: El manifold podría no estar conectado totalmente.\nEs decir, el manifold podría ser simplemente un montón de islas de puntos sin vecinos muy cercanos.\nSolución: Usar la conectividad local.\nEl algoritmo asume que el manifold es localmente conexo. Debido a la maldición de la dimensionalidad, los datos en un espacio de alta dimensión tienen una mayor distancia, pero también pueden ser más similares entre sí. Esto significa que la distancia al primer vecino más cercano puede ser bastante grande, pero la distancia al décimo vecino más cercano suele ser solo ligeramente mayor (relativamente hablando). La restricción de conectividad local asegura que nos centremos en la diferencia de distancia entre los vecinos más cercanos, no en la distancia absoluta (lo que muestra que la diferencia entre vecinos es pequeña).\nProblema 3: Incompatibilidad de la métrica local.\nCada punto tiene una métrica local asociada, y desde el punto de vista del punto \\(a\\), la distancia desde el punto a hasta el punto b puede ser 1.5, pero desde el punto de vista del punto \\(b\\), la distancia desde el punto b hasta el punto a podría ser solo 0.6.\nBasándonos en la intuición del gráfico, se puede considerar que esto es un borde dirigido con diferentes pesos, como se muestra en la siguiente figura:\n\nCombinar los dos bordes inconsistentes con pesos a y b juntos, entonces deberíamos tener un peso combinado \\(a+b-a\\cdot b\\). La forma de pensar esto es que el peso es en realidad la probabilidad de que exista el borde (1-símplex). Entonces, el peso combinado es la probabilidad de que exista al menos un borde.\nSi aplicamos este proceso para fusionar todos los conjuntos simpliciales difusos juntos, terminamos con un solo complejo simplicial difuso, que podemos considerar nuevamente como un gráfico ponderado. En términos de cálculo, simplemente aplicamos la fórmula de combinación de pesos de bordes a todo el gráfico (el peso de los no bordes es 0). Al final, obtenemos algo como esto.\n\nEntonces, asumiendo que ahora tenemos una representación topológica difusa de los datos (hablando matemáticamente, capturará la topología del manifold detrás de los datos), ¿cómo lo convertimos en una representación de baja dimensión?\n\n\n9.3.7 Encontrando una representación de baja dimensión\nLa representación de baja dimensión debe tener la misma estructura topologica fuzzy de los datos. Tenemos dos problemas acá: 1. Cómo determinar la representación fuzzy en el espacio de baja dimensión y 2. cómo encontrar una buena.\nPara 1., básicamente se haraá el mismo proceso pero con un espacio de \\(\\mathbb{R}^2\\) o \\(\\mathbb{R}^3\\).\nCon 2., el problema se resuelve calibrando las mismas distancias de la topología difusa en la variedad con respecto a la distancias de la topología en \\(\\mathbb{R}^{2}\\).\nRecordando el método de procesamiento de peso anterior, interpretamos el peso como la probabilidad de la existencia de un símplex. Dado que las dos topologías que estamos comparando comparten el mismo 0-símplex, es concebible que estamos comparando dos vectores de probabilidad indexados por el 1-símplex. Suponiendo que estos son todas variables de Bernoulli (el símplex final existe o no, y la probabilidad es un parámetro de la distribución de Bernoulli), la elección correcta aquí es la entropía cruzada.\nPara entender el proceso priero definamos algunos conceptos.\nUsando los \\(k\\) vecinos más cercanos para \\(x_i\\) es el conjunto de puntos \\(\\{x_{i_{1}}, \\dots, x_{i_{k}}\\}\\) tal que:\n\\[\n\\rho_{i} = \\min_{1 \\leq j \\leq k} d(x_{i}, x_{i_{j}})\n\\]\nLa función de peso para el 1-símplex \\(\\{x_{i}, x_{j}\\}\\) es:\n\\[\nw_{h}(x_{i}, x_{j}) = \\exp\\left(-\\dfrac{d(x_{i}, x_{j}) - \\rho_{i}}{\\sigma_{i}}\\right)\n\\]\nSi el conjunto de todos los posibles 1-símplexes entre \\(x_i\\) y \\(x_j\\) y la función ponderada hace que \\(w_h(x_{i}, x_{j})\\) sea el peso de ese simplex en la dimensión alta, y \\(w_l(x_i^{l}, x_j^{l})\\) en la dimensión baja, entonces la entropía cruzada es:\n\\[\n\\sum_{i=1}^{N} \\sum_{j=1}^{N} w_{h}(x_{i}, x_{j}) \\frac{\\log(w_{h}(x_{i}, x_{j}))}{\\log(w_{l}(x^{l}_{i}, x^{l}_{j}))} + (1-w_{h}(x_{i}, x_{j})) \\frac{\\log(1-w_{h}(x_{i}, x_{j}))}{\\log(1-w_{l}(x^{l}_{i}, x^{l}_{j}))}\n\\]\nDesde la perspectiva de los gráficos, minimizar la entropía cruzada se puede considerar como un algoritmo de diseño de gráficos dirigido por fuerza.\nEl primer ítem, \\(w_h(e) \\log(w_h(e)/w_l(e))\\) proporciona atracción entre los puntos \\(e\\) cuando hay un peso mayor en el espacio de alta dimensión. Al minimizar este sumando \\(w_l(e)\\) debe ser lo más grande posible y la distancia entre puntos es lo más pequeña posible.\nEl segundo sumando, \\((1 - w_h(e)) \\log((1 - w_h(e))/(1 - w_l(e)))\\) proporciona fuerza repulsiva entre los dos segmentos de \\(e\\) cuando \\(w_h(e)\\) es pequeño. Al hacer \\(w_l(e)\\) lo más pequeño posible, se minimiza esta parte.\n\n\n9.3.8 Ejemplos\n\nimport umap\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_digits\nimport pandas\n\n# Cargar el conjunto de datos\ndigits = load_digits()\ndata = digits.data\ntarget = digits.target\n\n# Instanciar UMAP y reducir la dimensionalidad\nreducer = umap.UMAP(random_state=42)\ndata_reduced = reducer.fit_transform(data)\ndata_reduced = pandas.DataFrame(data_reduced, columns=[\"x\", \"y\"])\n# Visualizar el resultado\n\n\nsns.scatterplot(data= data_reduced,x = \"x\", y=\"y\", hue=target, palette='tab10')\nplt.title('UMAP projection of the Digits dataset')\nplt.show()\n\n# Compare el ressultado con PCA\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\ndata_pca = pca.fit_transform(data)\ndata_pca = pandas.DataFrame(data_pca, columns=[\"x\", \"y\"])\n\nsns.scatterplot(data= data_pca,x = \"x\", y=\"y\", hue=target, palette='tab10')\nplt.title('PCA projection of the Digits dataset')\nplt.show()\n\n2024-02-12 14:56:06.076788: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n/usr/local/lib/python3.11/site-packages/umap/umap_.py:1943: UserWarning:\n\nn_jobs value -1 overridden to 1 by setting random_state. Use no seed for parallelism.\n\nOMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport umap.umap_ as umap\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.svm import SVC\n\n# Cargar el conjunto de datos\ndigits = load_digits()\ndata = digits.data\ntarget = digits.target\n\n# Dividir el conjunto de datos\nX_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.25, random_state=42)\n\n\n# Reducción de dimensionalidad con UMAP\numap_reducer = umap.UMAP(random_state=42)\nX_train_reduced = umap_reducer.fit_transform(X_train)\nX_test_reduced = umap_reducer.transform(X_test)\n\n# Claificación con SVM\nsvm = SVC()\nsvm.fit(X_train_reduced, y_train)\n\n# Predicción y evaluación\ny_pred = svm.predict(X_test_reduced)\nprint(\"Accuracy con UMAP:\", accuracy_score(y_test, y_pred))\n\n/usr/local/lib/python3.11/site-packages/umap/umap_.py:1943: UserWarning:\n\nn_jobs value -1 overridden to 1 by setting random_state. Use no seed for parallelism.\n\n\n\nAccuracy con UMAP: 0.9666666666666667\n\n\n\n# Clasificación con SVM\nsvm = SVC()\nsvm.fit(X_train, y_train)\n\n# Predicción y evaluación\ny_pred = svm.predict(X_test)\nprint(\"Accuracy sin UMAP:\", accuracy_score(y_test, y_pred))\n\nAccuracy sin UMAP: 0.9866666666666667\n\n\n\n# Reducción de dimensionalidad con PCA\npca = PCA(n_components=2)\nX_train_pca = pca.fit_transform(X_train)\nX_test_pca = pca.transform(X_test)\n\n# Clasificación con SVM\nsvm = SVC()\nsvm.fit(X_train_pca, y_train)\n\n# Predicción y evaluación\ny_pred = svm.predict(X_test_pca)\nprint(\"Accuracy con PCA:\", accuracy_score(y_test, y_pred))\n\nAccuracy con PCA: 0.6577777777777778",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Aprendizaje no supervisado</span>"
    ]
  }
]
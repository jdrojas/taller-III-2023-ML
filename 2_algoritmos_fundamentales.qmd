# Algoritmos fundamentales

## Regresión Lineal

## Regresión Logística

## Decision Trees

### Definición

Un árbol de decisión (DT) es un grafo no cíclico que se utiliza para tomar decisiones (clasificar). En cada nodo (rama) del grafo se evalúa uno de los *features*. Si el resultado de la evaluación es cierto (o está debajo de un umbral), se sigue la rama de la izquierda, si no se va a la derecha.

![](./figuras/DT01.svg){fig-alt="Un árbol de decisión." fig-align="center"}

Por lo tanto, los DT son un modelo no paramétrico.

Para crear el DT, se **intenta** optimizar el promedio de la máxima verosimilitud:
$$
    \frac{1}{N} \sum_{i=1}^{N}\left( y_i \ln{f_{ID3}(x_i)} + (1-y_i) \ln{(1-f_{ID3}(x_i))}\right)
$$
donde $f_{ID3}$ es un DT y $f_{ID3}(x) \stackrel{\text{def}}{=} Pr(y=1|x)$

### Construcción

Para construir el árbol, en cada nodo de decisión, se intenta minimizar la entropía de la información.

La entropía de un conjunto $\cal{S}$ viene dada por:
$$
 H(S) \stackrel{\text{def}}{=} -f_{ID3}^{S} \log_2 (f_{ID3}^{S}) - (1-f_{ID3}^{S}) \log_2 (1-f_{ID3}^{S})
$$

Y si un grupo se divide en dos, la entropía es la suma ponderada de cada subconjunto:
$$
  H(S_-, S_+) \stackrel{\text{def}}{=} \frac{|S_-|}{|S|}H(S_-) + \frac{|S_+|}{|S|}H(S_+)
$$

### Ejemplo

Consideremos los siguientes datos:

Atributos:

* Edad: viejo (v), media-vida(m), nuevo (nv)
* Competencia: no(n), sí(s)
* Tipo: software (swr), hardware (hwr)

| Edad | Competencia | Tipo | Ganancia |
|------|-------------|------|----------|
| v    | s           | swr  | baja     |
| v    | n           | swr  | baja     |
| v    | n           | hwr  | baja     |
| m    | s           | swr  | baja     |
| m    | s           | hwr  | baja     |
| m    | n           | hwr  | sube     |
| m    | n           | swr  | sube     |
| nv   | s           | swr  | sube     |
| nv   | n           | hwr  | sube     |
| nv   | n           | swr  | sube     |

Cálculo de las entropías: Primero se tiene que probar todos los features para ver cuál tiene mayor ganancia de información (reduce la entropía)

Entropía total:
$$
H(S) = \text{Entropía de los casos baja} + \text{Entropía de los casos sube}
$$

$$
H(s) = -\frac{5}{10}*\log_2(\frac{5}{10}) - \frac{5}{10}*\log_2(\frac{5}{10}) = 1
$$

Ahora vamos a decidir la primera separación con las edades
![](./figuras/DT02.svg){fig-align="center"}
$H = \frac{3}{10} \cdot 0 + \frac{4}{10} \cdot 1 + \frac{3}{10} \cdot 0 = 0.4$

Ahora vamos a decidir la primera separación con la competencia
![](./figuras/DT03.svg){fig-align="center"}
$H = \frac{4}{10} \cdot 0.811 + \frac{6}{10} \cdot 0.918 = 0.8752$

Ahora vamos a decidir la primera separación con las edades
![](./figuras/DT03.svg){fig-align="center"}
$H = \frac{4}{10} \cdot 0.811 + \frac{6}{10} \cdot 0.918 = 0.8752$

Ahora vamos a decidir la primera separación con el tipo
![](./figuras/DT04.svg){fig-align="center"}
$H = \frac{6}{10} \cdot 1 + \frac{4}{10} \cdot 1 = 1$

Concluimos que lo que nos da la máxima ganancia de información es primero decidir por edades, eso nos deja dos nodos hoja y un nodo rama que debemos volver a separar.

Ahora vamos a buscar el segundo nivel, donde vamos a separar el grupo que tiene edades medias por competencia:

![](./figuras/DT05.svg){fig-align="center"}
$H = \frac{2}{4} \cdot 0 + \frac{2}{4} \cdot 0 = 0$

Con esto ya se clasificaron todos los datos, puesto que terminamos solo con nodos hojas:
![](./figuras/DT06.svg){fig-align="center"}

Esto también se puede hacer con valores numéricos, que de hecho, es lo que se puede hacer con scikit learn
![](./figuras/DT11.svg){fig-align="center"}

y con esto se obtiene este árbol de decisión:
![](./figuras/DT12.svg){fig-align="center"}

### Comandos básicos en python

Estos son los comandos básicos en python

```python
#| label: dibujoArbol01
#| fig-cap: "Árbol de decisión"
from sklearn import tree
X = # Lista con los features (lista de listas)
Y = # Lista con los labels
# Se define la variable que tendrá el árbol
clf = tree.DecisionTreeClassifier()
# Se calcula el árbol
clf = clf.fit(X, Y)
# Se utiliza el árbol para predecir el label de un dato nuevo
clf.predict_proba(X0)
# Se dibuja el árbol
tree.plot_tree(clf)
```

y este sería un ejemplo sencillo en python:

Primero creamos los datos

```{python}
#| label: ejemplo01Datos
#| fig-cap: "Ejemplo hecho en python: datos"

from sklearn import tree
from sklearn.datasets import make_blobs
from sklearn.inspection import DecisionBoundaryDisplay
import matplotlib.pyplot as plt
import numpy as np

# Creación de los datos
X, Y = make_blobs(n_samples=200, centers=4, random_state=6)
plt.scatter(X[:, 0], X[:, 1], c=Y, s=30)
plt.title("Datos originales")
plt.xlabel("x1")
plt.ylabel("x2")
plt.show()
```

Luego se crea el arbol

```{python}
#| label: ejemplo01Arbol
#| fig-cap: "Ejemplo hecho en python: arbol"
clf = tree.DecisionTreeClassifier()
clf = clf.fit(X, Y)
tree.plot_tree(clf)
plt.show()
```

y por último, dibujamos las separaciones

```{python}
#| label: ejemplo01Separacion
#| fig-cap: "Ejemplo hecho en python: separación"
DecisionBoundaryDisplay.from_estimator(clf, X, response_method="predict")
plt.scatter(X[:, 0], X[:, 1], c=Y, s=30)
plt.show()
```

y con esto se puede aplicar el árbol

```{python}
#| label: ejemplo01Aplicacion
print(clf.predict([[5.0, 1.0]]))
print(clf.predict([[-2.0, -1.0]]))
print(clf.predict([[6.0, -6.0]]))
```

y lo que devuelve es el número de grupo al que pertene el dato

## Máquinas de Soporte Vectorial

El problema con SVM es que ocurre si los datos no se pueden separar con un hiperplano.

Recordemos que en SVM, se deben satisfacer las siguientes condiciones:
a. \(wx_i-b \geq 1\) si \(y_i = 1\), y
b. \(wx_i-b \leq -1\) si \(y_i = -1\)

Y la función a minimizar es:

\begin{equation*}
    \min_{w,b} \frac{1}{2}||w||^2 \text{ sujeto a } y_i(wx_i-b) \geq 1 \text{ para } i=1,2,\ldots,N.
\end{equation*}

### El truco del kernel

El truco del kernel consiste en transformar los datos a un espacio de mayor dimensión, donde se pueda separar con un hiperplano. El truco consiste en definir una función \(\phi\) tal que \(\phi: x\to \phi(x)\), donde \(\phi(x)\)
 es un vector de dimensión superior.

 El problema de minimización se puede reescribir como:

 \begin{equation*}
\max_{\alpha_1, \dots, \alpha_N} \sum_{i=1}^{N} \alpha_i - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} y_i \alpha_i <x_i, x_j> y_j \alpha_j
\end{equation*}

sujeto a que \(\alpha_i \geq 0\) y \(\sum_{i=1}^{N} \alpha_i y_i = 0\), donde \(<x_i, x_j>\) es el producto punto entre los vectores \(x_i\) y \(x_j\).

En este caso los kernels \(<x_i, x_j>\) se pueden definir como:
* Lineal: \(<x_i, x_j> = x_i^T x_j\)
* Polinomial: \(<x_i, x_j> = (\gamma x_i^T x_j + r)^d\)
* Gaussiano o RBF:  \(<x_i, x_j> = \exp{(-\gamma \Vert x_{i} - x_{j}\Vert^2)}\)
* Sigmoide: \(<x_i, x_j> = \tanh{(\gamma x_i^T x_j + r)}\)

```{python}
import matplotlib.pyplot as plt
import numpy as np

X = np.array(
    [
        [0.4, -0.7],
        [-1.5, -1.0],
        [-1.4, -0.9],
        [-1.3, -1.2],
        [-1.1, -0.2],
        [-1.2, -0.4],
        [-0.5, 1.2],
        [-1.5, 2.1],
        [1.0, 1.0],
        [1.3, 0.8],
        [1.2, 0.5],
        [0.2, -2.0],
        [0.5, -2.4],
        [0.2, -2.3],
        [0.0, -2.7],
        [1.3, 2.1],
    ]
)

y = np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1])

# Plotting settings
fig, ax = plt.subplots(figsize=(4, 3))
x_min, x_max, y_min, y_max = -3, 3, -3, 3
ax.set(xlim=(x_min, x_max), ylim=(y_min, y_max))

# Plot samples by color and add legend
scatter = ax.scatter(X[:, 0], X[:, 1], s=150, c=y, label=y, edgecolors="k")
ax.legend(*scatter.legend_elements(), loc="upper right", title="Classes")
ax.set_title("Samples in two-dimensional feature space")
_ = plt.show()
```

```{python}
from sklearn import svm
from sklearn.inspection import DecisionBoundaryDisplay


def plot_training_data_with_decision_boundary(kernel):
    # Train the SVC
    clf = svm.SVC(kernel=kernel, gamma=2).fit(X, y)

    # Settings for plotting
    _, ax = plt.subplots(figsize=(4, 3))
    x_min, x_max, y_min, y_max = -3, 3, -3, 3
    ax.set(xlim=(x_min, x_max), ylim=(y_min, y_max))

    # Plot decision boundary and margins
    common_params = {"estimator": clf, "X": X, "ax": ax}
    DecisionBoundaryDisplay.from_estimator(
        **common_params,
        response_method="predict",
        plot_method="pcolormesh",
        alpha=0.3,
    )
    DecisionBoundaryDisplay.from_estimator(
        **common_params,
        response_method="decision_function",
        plot_method="contour",
        levels=[-1, 0, 1],
        colors=["k", "k", "k"],
        linestyles=["--", "-", "--"],
    )

    # Plot bigger circles around samples that serve as support vectors
    ax.scatter(
        clf.support_vectors_[:, 0],
        clf.support_vectors_[:, 1],
        s=250,
        facecolors="none",
        edgecolors="k",
    )
    # Plot samples by color and add legend
    ax.scatter(X[:, 0], X[:, 1], c=y, s=150, edgecolors="k")
    ax.legend(*scatter.legend_elements(), loc="upper right", title="Classes")
    ax.set_title(f" Decision boundaries of {kernel} kernel in SVC")

    _ = plt.show()

```

```{python}
plot_training_data_with_decision_boundary("linear")
```

```{python}
plot_training_data_with_decision_boundary("poly")
```

```{python}
plot_training_data_with_decision_boundary("rbf")
```

```{python}
plot_training_data_with_decision_boundary("sigmoid")
```

## K-Nearest Neighbors (KNN)

### Carga de paquetes

```{python}
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.inspection import DecisionBoundaryDisplay

import pandas as pd
import seaborn as sns
from sklearn.datasets import make_blobs
```

# Cargas datos

```{python}
X, y = make_blobs(n_samples=1000, centers=3, random_state=6)
```

# Visualizar los datos

```{python}
sns.scatterplot(x=X[:,0],y=X[:,1], hue=y)
plt.show()
```

# Se normaliza y se divide los datos

```{python}


# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)

# Scale the features using StandardScaler
scaler = StandardScaler()
X = scaler.fit_transform(X)
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
```

# Ajuste y evaluación del modelo

```{python}
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)

# predecir con el modelo
y_pred = knn.predict(X_test)

# evaluarlo
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
```

```{python}
DecisionBoundaryDisplay.from_estimator(knn, X_train)
sns.scatterplot(x=X[:,0],y=X[:,1], hue=y)
plt.show()
```
